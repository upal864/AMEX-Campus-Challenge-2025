{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3173b-b24a-461b-a48e-c9a4f5ddb4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n",
    "# pip install tqdm\n",
    "# pip install ipywidgets\n",
    "# pip install \"numpy<2.0\"\n",
    "# pip install --upgrade scipy scikit-learn\n",
    "# pip install pyarrow pandas\n",
    "# pip install lightgbm matplotlib\n",
    "# pip install optuna\n",
    "# pip install -U sentence-transformers\n",
    "# pip install \\\n",
    "#     --extra-index-url=https://pypi.nvidia.com \\\n",
    "#     \"cudf-cu12==25.6.\" \"dask-cudf-cu12==25.6.\" \"cuml-cu12==25.6.*\" \\\n",
    "#     \"cugraph-cu12==25.6.\" \"nx-cugraph-cu12==25.6.\" \"cuxfilter-cu12==25.6.*\" \\\n",
    "#     \"cucim-cu12==25.6.\" \"pylibraft-cu12==25.6.\" \"raft-dask-cu12==25.6.*\" \\\n",
    "#     \"cuvs-cu12==25.6.\" \"nx-cugraph-cu12==25.6.\"\n",
    "# curl -L https://lambdalabs-guest-agent.s3.us-west-2.amazonaws.com/scripts/install.sh | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f185bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import warnings\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import gc\n",
    "# import time\n",
    "\n",
    "# # --- Configuration ---\n",
    "# warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# # --- File Paths ---\n",
    "# # Assumes data is in the same directory as the scriptQ\n",
    "# RAW_DATA_PATHS = {\n",
    "#     \"train\": \"train_data.parquet\",\n",
    "#     \"test\": \"test_data_r3.parquet\",\n",
    "#     \"add_trans\": \"add_trans.parquet\",\n",
    "#     \"add_event\": \"add_event.parquet\",\n",
    "#     \"offer_meta\": \"offer_metadata.parquet\",\n",
    "# }\n",
    "# OUTPUT_DIR = \"inter\"\n",
    "# OUTPUT_PATHS = {\n",
    "#     \"train\": os.path.join(OUTPUT_DIR, \"train_enriched.parquet\"),\n",
    "#     \"test\": os.path.join(OUTPUT_DIR, \"test_enriched.parquet\"),\n",
    "# }\n",
    "\n",
    "# # --- Helper Functions ---\n",
    "\n",
    "\n",
    "# def load_raw_data(paths):\n",
    "#     \"\"\"Loads all necessary raw data files from parquet format.\"\"\"\n",
    "#     print(\"1/7: Loading all raw data files...\")\n",
    "#     data = {}\n",
    "#     try:\n",
    "#         for name, path in paths.items():\n",
    "#             print(f\"  -> Loading {path}...\")\n",
    "#             data[name] = pd.read_parquet(path)\n",
    "#     except FileNotFoundError as e:\n",
    "#         print(f\"❌ ERROR: Could not find a required data file. Details: {e}\")\n",
    "#         print(\n",
    "#             \"Please ensure all .parquet files are in the same directory as the script.\"\n",
    "#         )\n",
    "#         exit()\n",
    "#     return data\n",
    "\n",
    "\n",
    "# def combine_and_preprocess(train_df, test_df):\n",
    "#     \"\"\"Combines train and test sets and performs initial preprocessing.\"\"\"\n",
    "#     print(\"2/7: Combining data and processing timestamps...\")\n",
    "#     train_len = len(train_df)\n",
    "\n",
    "#     all_data = pd.concat([train_df, test_df], ignore_index=True)\n",
    "#     del train_df, test_df\n",
    "#     gc.collect()\n",
    "\n",
    "#     # Convert main interaction timestamp\n",
    "#     all_data[\"id4\"] = pd.to_datetime(all_data[\"id4\"])\n",
    "\n",
    "#     return all_data, train_len\n",
    "\n",
    "\n",
    "# def engineer_offer_features(all_data, offer_meta):\n",
    "#     \"\"\"Engineers static and basic NLP features from offer metadata.\"\"\"\n",
    "#     print(\"3/7: Engineering static and basic NLP offer features...\")\n",
    "\n",
    "#     # --- Timestamp and Duration Features ---\n",
    "#     offer_meta[\"id12\"] = pd.to_datetime(offer_meta[\"id12\"])\n",
    "#     offer_meta[\"id13\"] = pd.to_datetime(offer_meta[\"id13\"])\n",
    "#     offer_meta[\"offer_duration_days\"] = (\n",
    "#         offer_meta[\"id13\"] - offer_meta[\"id12\"]\n",
    "#     ).dt.days\n",
    "#     offer_meta[\"offer_start_dayofweek\"] = offer_meta[\"id12\"].dt.dayofweek\n",
    "\n",
    "#     # --- Categorical and Renaming ---\n",
    "#     offer_meta[\"brand_id\"], _ = pd.factorize(offer_meta[\"id11\"])\n",
    "#     offer_meta[\"industry_id\"], _ = pd.factorize(offer_meta[\"id10\"])\n",
    "#     offer_meta.rename(\n",
    "#         columns={\n",
    "#             \"f375\": \"redemption_frequency\",\n",
    "#             \"f376\": \"discount_rate\",\n",
    "#             \"id8\": \"member_industry_code\",\n",
    "#             \"id9\": \"offer_name\",\n",
    "#             \"f378\": \"offer_body\",\n",
    "#         },\n",
    "#         inplace=True,\n",
    "#     )\n",
    "#     offer_meta[\"is_industry_match\"] = (\n",
    "#         offer_meta[\"industry_id\"] == offer_meta[\"member_industry_code\"]\n",
    "#     ).astype(int)\n",
    "\n",
    "#     # --- Basic NLP Features ---\n",
    "#     print(\"  -> Generating basic NLP features from offer text...\")\n",
    "#     offer_meta[\"offer_name\"] = (\n",
    "#         offer_meta[\"offer_name\"].astype(str).str.lower().fillna(\"\")\n",
    "#     )\n",
    "#     offer_meta[\"offer_body\"] = (\n",
    "#         offer_meta[\"offer_body\"].astype(str).str.lower().fillna(\"\")\n",
    "#     )\n",
    "#     offer_meta[\"offer_name_len\"] = offer_meta[\"offer_name\"].str.len()\n",
    "#     offer_meta[\"offer_body_words\"] = offer_meta[\"offer_body\"].str.split().str.len()\n",
    "\n",
    "#     # Keyword matching\n",
    "#     offer_meta[\"has_keyword_cashback\"] = (\n",
    "#         offer_meta[\"offer_body\"]\n",
    "#         .str.contains(\"cash back|statement credit|refund\", regex=True)\n",
    "#         .astype(int)\n",
    "#     )\n",
    "#     offer_meta[\"has_keyword_points\"] = (\n",
    "#         offer_meta[\"offer_body\"]\n",
    "#         .str.contains(\"points|miles|rewards\", regex=True)\n",
    "#         .astype(int)\n",
    "#     )\n",
    "#     offer_meta[\"has_keyword_discount\"] = (\n",
    "#         offer_meta[\"offer_body\"]\n",
    "#         .str.contains(\"% off|discount|save\", regex=True)\n",
    "#         .astype(int)\n",
    "#     )\n",
    "#     offer_meta[\"has_keyword_spend_x\"] = (\n",
    "#         offer_meta[\"offer_body\"].str.contains(\"spend\", regex=False).astype(int)\n",
    "#     )\n",
    "\n",
    "#     # --- Merge into Main DataFrame ---\n",
    "#     offer_features_to_merge = [\n",
    "#         \"id3\",\n",
    "#         \"offer_duration_days\",\n",
    "#         \"offer_start_dayofweek\",\n",
    "#         \"brand_id\",\n",
    "#         \"industry_id\",\n",
    "#         \"redemption_frequency\",\n",
    "#         \"discount_rate\",\n",
    "#         \"is_industry_match\",\n",
    "#         \"offer_name_len\",\n",
    "#         \"offer_body_words\",\n",
    "#         \"has_keyword_cashback\",\n",
    "#         \"has_keyword_points\",\n",
    "#         \"has_keyword_discount\",\n",
    "#         \"has_keyword_spend_x\",\n",
    "#         \"id12\",\n",
    "#         \"id13\",  # Keep timestamps for interaction feature creation\n",
    "#     ]\n",
    "\n",
    "#     all_data[\"id3\"] = all_data[\"id3\"].astype(\"int64\")\n",
    "#     offer_meta[\"id3\"] = offer_meta[\"id3\"].astype(\"int64\")\n",
    "#     all_data = all_data.merge(offer_meta[offer_features_to_merge], on=\"id3\", how=\"left\")\n",
    "\n",
    "#     # Create time-based interaction features\n",
    "#     all_data[\"days_since_offer_start\"] = (all_data[\"id4\"] - all_data[\"id12\"]).dt.days\n",
    "#     all_data[\"days_until_offer_end\"] = (all_data[\"id13\"] - all_data[\"id4\"]).dt.days\n",
    "#     all_data.drop(columns=[\"id12\", \"id13\"], inplace=True)\n",
    "\n",
    "#     return all_data\n",
    "\n",
    "\n",
    "# def engineer_event_history_features(all_data, add_event, offer_meta):\n",
    "#     \"\"\"Engineers historical customer-offer interaction features, preventing data leakage.\"\"\"\n",
    "#     print(\"4/7: Engineering customer-offer historical interaction features...\")\n",
    "\n",
    "#     # --- Preprocessing ---\n",
    "#     add_event[\"id4\"] = pd.to_datetime(add_event[\"id4\"])\n",
    "#     add_event[\"id7\"] = pd.to_datetime(\n",
    "#         add_event[\"id7\"], errors=\"coerce\"\n",
    "#     )  # Click timestamp can be NaT\n",
    "#     add_event[\"id3\"] = add_event[\"id3\"].astype(\"int64\")\n",
    "\n",
    "#     # Merge brand/industry info needed for grouped features\n",
    "#     add_event = add_event.merge(\n",
    "#         offer_meta[[\"id3\", \"brand_id\", \"industry_id\"]], on=\"id3\", how=\"left\"\n",
    "#     )\n",
    "#     add_event.sort_values(by=[\"id2\", \"id4\"], inplace=True)\n",
    "\n",
    "#     # --- Feature Creation ---\n",
    "#     add_event[\"clicked\"] = add_event[\"id7\"].notna().astype(int)\n",
    "\n",
    "#     # Time-since-last-event features\n",
    "#     add_event[\"time_since_last_impression_seconds\"] = (\n",
    "#         add_event.groupby(\"id2\")[\"id4\"].diff().dt.total_seconds()\n",
    "#     )\n",
    "#     add_event[\"click_timestamp\"] = add_event[\"id4\"].where(add_event[\"clicked\"] == 1)\n",
    "#     add_event[\"last_click_timestamp\"] = add_event.groupby(\"id2\")[\n",
    "#         \"click_timestamp\"\n",
    "#     ].ffill()\n",
    "#     add_event[\"time_since_last_click_seconds\"] = (\n",
    "#         add_event[\"id4\"] - add_event[\"last_click_timestamp\"]\n",
    "#     ).dt.total_seconds()\n",
    "\n",
    "#     # CRITICAL: Lagged cumulative counts to prevent data leakage from the current event\n",
    "#     add_event[\"customer_total_impressions_before\"] = add_event.groupby(\"id2\").cumcount()\n",
    "#     add_event[\"customer_brand_impressions_before\"] = add_event.groupby(\n",
    "#         [\"id2\", \"brand_id\"]\n",
    "#     ).cumcount()\n",
    "#     add_event[\"customer_total_clicks_before\"] = (\n",
    "#         add_event.groupby(\"id2\")[\"clicked\"].cumsum().shift(1).fillna(0)\n",
    "#     )\n",
    "#     add_event[\"customer_brand_clicks_before\"] = (\n",
    "#         add_event.groupby([\"id2\", \"brand_id\"])[\"clicked\"].cumsum().shift(1).fillna(0)\n",
    "#     )\n",
    "\n",
    "#     # Historical Click-Through-Rates (CTR)\n",
    "#     epsilon = 1e-6\n",
    "#     add_event[\"customer_ctr_before\"] = add_event[\"customer_total_clicks_before\"] / (\n",
    "#         add_event[\"customer_total_impressions_before\"] + epsilon\n",
    "#     )\n",
    "#     add_event[\"customer_brand_ctr_before\"] = add_event[\n",
    "#         \"customer_brand_clicks_before\"\n",
    "#     ] / (add_event[\"customer_brand_impressions_before\"] + epsilon)\n",
    "\n",
    "#     # --- Merge into Main DataFrame ---\n",
    "#     event_key = [\"id2\", \"id3\", \"id4\"]\n",
    "#     event_features_to_merge = [\n",
    "#         \"time_since_last_impression_seconds\",\n",
    "#         \"time_since_last_click_seconds\",\n",
    "#         \"customer_total_impressions_before\",\n",
    "#         \"customer_brand_impressions_before\",\n",
    "#         \"customer_total_clicks_before\",\n",
    "#         \"customer_brand_clicks_before\",\n",
    "#         \"customer_ctr_before\",\n",
    "#         \"customer_brand_ctr_before\",\n",
    "#     ]\n",
    "\n",
    "#     for col in event_key:\n",
    "#         all_data[col] = all_data[col].astype(add_event[col].dtype)\n",
    "\n",
    "#     all_data = all_data.merge(\n",
    "#         add_event[event_key + event_features_to_merge], on=event_key, how=\"left\"\n",
    "#     )\n",
    "\n",
    "#     return all_data\n",
    "\n",
    "\n",
    "# def engineer_transaction_history_features(all_data, add_trans):\n",
    "#     \"\"\"Engineers point-in-time customer transaction features using merge_asof.\"\"\"\n",
    "#     print(\"5/7: Engineering POINT-IN-TIME customer transaction features...\")\n",
    "\n",
    "#     # --- Preprocessing ---\n",
    "#     add_trans[\"f370\"] = pd.to_datetime(add_trans[\"f370\"])\n",
    "#     time_as_seconds = pd.to_numeric(add_trans[\"f371\"], errors=\"coerce\")\n",
    "#     add_trans[\"transaction_timestamp\"] = add_trans[\"f370\"] + pd.to_timedelta(\n",
    "#         time_as_seconds, unit=\"s\"\n",
    "#     )\n",
    "#     add_trans.sort_values(by=[\"id2\", \"transaction_timestamp\"], inplace=True)\n",
    "\n",
    "#     # --- Build Point-in-Time Aggregates ---\n",
    "#     print(\"  -> Calculating point-in-time cumulative aggregates...\")\n",
    "#     trans_history = add_trans[\n",
    "#         [\"id2\", \"transaction_timestamp\", \"f367\", \"f368\", \"f369\"]\n",
    "#     ].copy()\n",
    "\n",
    "#     is_debit = trans_history[\"f369\"] == \"D\"\n",
    "#     debit_amount = trans_history[\"f367\"].where(is_debit, 0)\n",
    "#     credit_amount = trans_history[\"f367\"].where(trans_history[\"f369\"] == \"C\", 0)\n",
    "\n",
    "#     grouped = trans_history.groupby(\"id2\")\n",
    "    \n",
    "#     # === FIX (Warning): Added include_groups=False to silence FutureWarning ===\n",
    "#     trans_history[\"customer_total_spend\"] = grouped.apply(\n",
    "#         lambda x: debit_amount[x.index].cumsum(), include_groups=False\n",
    "#     ).reset_index(level=0, drop=True)\n",
    "#     trans_history[\"customer_total_refund_amount\"] = grouped.apply(\n",
    "#         lambda x: credit_amount[x.index].cumsum(), include_groups=False\n",
    "#     ).reset_index(level=0, drop=True)\n",
    "#     trans_history[\"customer_num_transactions\"] = grouped.cumcount() + 1\n",
    "#     trans_history[\"customer_num_debits\"] = grouped.apply(\n",
    "#         lambda x: (x[\"f369\"] == \"D\").cumsum(), include_groups=False\n",
    "#     ).reset_index(level=0, drop=True)\n",
    "#     trans_history[\"customer_num_refunds\"] = grouped.apply(\n",
    "#         lambda x: (x[\"f369\"] == \"C\").cumsum(), include_groups=False\n",
    "#     ).reset_index(level=0, drop=True)\n",
    "\n",
    "#     # Forward-fill to handle groups with no debits/credits initially\n",
    "#     for col in [\n",
    "#         \"customer_total_spend\",\n",
    "#         \"customer_total_refund_amount\",\n",
    "#         \"customer_num_debits\",\n",
    "#         \"customer_num_refunds\",\n",
    "#     ]:\n",
    "#         trans_history[col] = grouped[col].ffill().fillna(0)\n",
    "\n",
    "#     epsilon = 1e-6\n",
    "#     trans_history[\"customer_avg_trans_amount\"] = trans_history[\n",
    "#         \"customer_total_spend\"\n",
    "#     ] / (trans_history[\"customer_num_debits\"] + epsilon)\n",
    "#     trans_history[\"customer_refund_rate\"] = trans_history[\"customer_num_refunds\"] / (\n",
    "#         trans_history[\"customer_num_debits\"] + epsilon\n",
    "#     )\n",
    "\n",
    "#     # === FIX (Error): Correctly apply expanding nunique on a grouped object ===\n",
    "#     # trans_history[\"customer_num_unique_products\"] = grouped[\"f368\"].apply(\n",
    "#     #     lambda s: pd.Series(pd.factorize(s)[0], index=s.index).expanding().nunique()\n",
    "#     # ).reset_index(level=0, drop=True)\n",
    "#     # === END FIX (Error) ===\n",
    "\n",
    "#     # --- Merge into Main DataFrame using merge_asof ---\n",
    "#     print(\"  -> Merging point-in-time features into main dataset...\")\n",
    "#     all_data.sort_values(by=\"id4\", inplace=True)\n",
    "#     trans_history.dropna(subset=[\"id2\", \"transaction_timestamp\"], inplace=True)\n",
    "\n",
    "#     features_to_merge = [\n",
    "#         \"id2\",\n",
    "#         \"transaction_timestamp\",\n",
    "#         \"customer_total_spend\",\n",
    "#         \"customer_num_transactions\",\n",
    "#         \"customer_avg_trans_amount\",\n",
    "#         # \"customer_num_unique_products\",\n",
    "#         \"customer_total_refund_amount\",\n",
    "#         \"customer_num_refunds\",\n",
    "#         \"customer_refund_rate\",\n",
    "#     ]\n",
    "\n",
    "#     all_data = pd.merge_asof(\n",
    "#         all_data,\n",
    "#         trans_history[features_to_merge],\n",
    "#         left_on=\"id4\",\n",
    "#         right_on=\"transaction_timestamp\",\n",
    "#         by=\"id2\",\n",
    "#         direction=\"backward\",  # CRITICAL for point-in-time correctness\n",
    "#     )\n",
    "\n",
    "#     all_data[\"days_since_last_transaction\"] = (\n",
    "#         all_data[\"id4\"] - all_data[\"transaction_timestamp\"]\n",
    "#     ).dt.days\n",
    "#     all_data.drop(columns=[\"transaction_timestamp\"], inplace=True)\n",
    "\n",
    "#     return all_data\n",
    "\n",
    "\n",
    "# def finalize_and_impute(all_data, train_len):\n",
    "#     \"\"\"Splits data back to train/test and handles NaN imputation.\"\"\"\n",
    "#     print(\"6/7: Finalizing datasets and imputing missing values...\")\n",
    "\n",
    "#     train_df = all_data.iloc[:train_len].copy()\n",
    "#     test_df = all_data.iloc[train_len:].copy()\n",
    "#     del all_data\n",
    "#     gc.collect()\n",
    "\n",
    "#     # Define columns for each imputation strategy\n",
    "#     time_cols = [c for c in train_df.columns if \"seconds\" in c or \"days\" in c]\n",
    "#     rate_cols = [c for c in train_df.columns if \"rate\" in c or \"ctr\" in c]\n",
    "#     count_cols = [\n",
    "#         c\n",
    "#         for c in train_df.columns\n",
    "#         if \"num_\" in c\n",
    "#         or \"total_\" in c\n",
    "#         or \"_before\" in c\n",
    "#         or \"_spend\" in c\n",
    "#         or \"_amount\" in c\n",
    "#     ]\n",
    "#     id_cols = [c for c in train_df.columns if \"brand_id\" in c or \"industry_id\" in c]\n",
    "\n",
    "#     # Get remaining numeric columns for default filling\n",
    "#     all_filled_cols = set(time_cols + rate_cols + count_cols + id_cols)\n",
    "#     other_numeric_cols = [\n",
    "#         c\n",
    "#         for c in train_df.select_dtypes(include=np.number).columns\n",
    "#         if c not in all_filled_cols and c not in [\"id\", \"id2\", \"id3\", \"y\"]\n",
    "#     ]\n",
    "\n",
    "#     print(\"  -> Imputing NaN values with strategy-based fillers...\")\n",
    "#     for df in [train_df, test_df]:\n",
    "#         df[time_cols] = df[time_cols].fillna(-999)\n",
    "#         df[rate_cols] = df[rate_cols].fillna(0.0)\n",
    "#         df[count_cols] = df[count_cols].fillna(0)\n",
    "#         df[id_cols] = df[id_cols].fillna(-1)\n",
    "#         df[other_numeric_cols] = df[other_numeric_cols].fillna(-1)\n",
    "\n",
    "#     return train_df, test_df\n",
    "\n",
    "\n",
    "# def save_data(train_df, test_df, paths):\n",
    "#     \"\"\"Saves the final dataframes to parquet files.\"\"\"\n",
    "#     print(\"7/7: Saving final datasets to disk...\")\n",
    "#     os.makedirs(os.path.dirname(paths[\"train\"]), exist_ok=True)\n",
    "#     train_df.to_parquet(paths[\"train\"], index=False)\n",
    "#     test_df.to_parquet(paths[\"test\"], index=False)\n",
    "#     print(f\"✅ Saved enriched train data to '{paths['train']}'\")\n",
    "#     print(f\"✅ Saved enriched test data to '{paths['test']}'\")\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"Main function to run the entire feature engineering pipeline.\"\"\"\n",
    "#     print(\"--- Starting Advanced Feature Engineering Pipeline (Fixed) ---\")\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # 1. Load Data\n",
    "#     raw_data = load_raw_data(RAW_DATA_PATHS)\n",
    "\n",
    "#     # 2. Combine & Preprocess\n",
    "#     all_data, train_len = combine_and_preprocess(raw_data[\"train\"], raw_data[\"test\"])\n",
    "\n",
    "#     # 3. Engineer Offer Features\n",
    "#     all_data = engineer_offer_features(all_data, raw_data[\"offer_meta\"])\n",
    "\n",
    "#     # 4. Engineer Event History Features\n",
    "#     all_data = engineer_event_history_features(\n",
    "#         all_data, raw_data[\"add_event\"], raw_data[\"offer_meta\"]\n",
    "#     )\n",
    "#     del raw_data[\"add_event\"]\n",
    "#     gc.collect()\n",
    "\n",
    "#     # 5. Engineer Transaction History Features\n",
    "#     all_data = engineer_transaction_history_features(all_data, raw_data[\"add_trans\"])\n",
    "#     del raw_data[\"add_trans\"]\n",
    "#     gc.collect()\n",
    "\n",
    "#     # 6. Finalize, Impute, and Split\n",
    "#     train_enriched, test_enriched = finalize_and_impute(all_data, train_len)\n",
    "\n",
    "#     # 7. Save Data\n",
    "#     save_data(train_enriched, test_enriched, OUTPUT_PATHS)\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     print(f\"\\n--- Initial Feature Engineering Complete! ---\")\n",
    "#     print(f\"Total time taken: {((end_time - start_time) / 60):.2f} minutes\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow.parquet as pq\n",
    "# import pandas as pd\n",
    "# import warnings\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import gc\n",
    "# import time\n",
    "\n",
    "# def run_stage_1_train_val_split():\n",
    "#     TRAIN_INPUT = \"inter/train_enriched.parquet\"\n",
    "#     TEST_INPUT = \"inter/test_enriched.parquet\"\n",
    "#     INTER_DIR = \"inter\"\n",
    "#     BATCH_SIZE = 100_000\n",
    "#     VAL_RATIO = 0.15\n",
    "\n",
    "#     print(\"\\n**Script 0: Train-Val Split (Customer-Aware & Time-Based)**\")\n",
    "#     os.makedirs(INTER_DIR, exist_ok=True)\n",
    "\n",
    "#     parquet_file = pq.ParquetFile(TRAIN_INPUT)\n",
    "\n",
    "#     # 1. Load IDs and timestamps to determine the split\n",
    "#     print(\"1/5: Loading IDs and timestamps to determine validation customers...\")\n",
    "#     id_chunks = []\n",
    "#     # CRITICAL FIX: Load customer ID 'id2' to ensure customer-based split\n",
    "#     for batch in parquet_file.iter_batches(\n",
    "#         batch_size=BATCH_SIZE, columns=[\"id2\", \"id4\"]\n",
    "#     ):\n",
    "#         chunk_df = batch.to_pandas()\n",
    "#         id_chunks.append(chunk_df)\n",
    "\n",
    "#     id_df = pd.concat(id_chunks, ignore_index=True)\n",
    "#     print(f\"1/5 ID index DataFrame created with shape: {id_df.shape}\")\n",
    "#     del id_chunks\n",
    "#     gc.collect()\n",
    "\n",
    "#     id_df[\"id4\"] = pd.to_datetime(id_df[\"id4\"])\n",
    "#     id_df.sort_values(by=\"id4\", inplace=True)\n",
    "\n",
    "#     # 2. Identify validation customers based on time\n",
    "#     # This ensures that the validation set is from a later time period than the training set.\n",
    "#     n_rows = len(id_df)\n",
    "#     split_idx = int(n_rows * (1 - VAL_RATIO))\n",
    "#     valid_customer_ids = set(id_df[\"id2\"].iloc[split_idx:])\n",
    "#     print(\n",
    "#         f\"2/5: Identified {len(valid_customer_ids)} customers for the time-based validation set.\"\n",
    "#     )\n",
    "#     del id_df\n",
    "#     gc.collect()\n",
    "\n",
    "#     # 3. Stream data and split into train/valid based on the identified customer IDs\n",
    "#     print(\"3/5: Splitting full dataset based on validation customer IDs...\")\n",
    "#     train_rows = []\n",
    "#     valid_rows = []\n",
    "\n",
    "#     for batch in parquet_file.iter_batches(batch_size=BATCH_SIZE):\n",
    "#         batch_df = batch.to_pandas()\n",
    "#         is_valid_customer = batch_df[\"id2\"].isin(valid_customer_ids)\n",
    "#         valid_rows.append(batch_df[is_valid_customer])\n",
    "#         train_rows.append(batch_df[~is_valid_customer])\n",
    "#     del parquet_file, valid_customer_ids\n",
    "\n",
    "#     # 4. Save the split datasets\n",
    "#     train_split = pd.concat(train_rows, ignore_index=True)\n",
    "#     train_split.to_parquet(f\"{INTER_DIR}/train_0.parquet\", index=False)\n",
    "#     print(f\"3/5 Saved train_0.parquet with shape: {train_split.shape}\")\n",
    "#     del train_split, train_rows\n",
    "#     gc.collect()\n",
    "\n",
    "#     valid_split = pd.concat(valid_rows, ignore_index=True)\n",
    "#     valid_split.to_parquet(f\"{INTER_DIR}/valid_0.parquet\", index=False)\n",
    "#     print(f\"4/5 Saved valid_0.parquet with shape: {valid_split.shape}\")\n",
    "#     del valid_split, valid_rows\n",
    "#     gc.collect()\n",
    "\n",
    "#     # 5. Copy test data for consistency\n",
    "#     test_df = pd.read_parquet(TEST_INPUT)\n",
    "#     test_df.to_parquet(f\"{INTER_DIR}/test_0.parquet\", index=False)\n",
    "#     print(f\"5/5 Saved test_0.parquet with shape: {test_df.shape}\")\n",
    "#     del test_df\n",
    "#     gc.collect()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Stage 1: Split enriched training data into train/validation sets\n",
    "#     run_stage_1_train_val_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54744ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DICT_PATH = \"data_dictionary.csv\"\n",
    "INPUT_DIR = \"inter\"\n",
    "OUTPUT_DIR = \"inter\"\n",
    "\n",
    "\n",
    "def downcast_numerical_dtypes(df):\n",
    "    \"\"\"\n",
    "    Intelligently downcasts numerical columns to save memory.\n",
    "    Finds float64 and int64 columns and converts them to the smallest possible format.\n",
    "    \"\"\"\n",
    "    print(\"   -> Downcasting numerical dtypes for memory optimization...\")\n",
    "    # Downcast floats\n",
    "    float_cols = df.select_dtypes(include=[\"float64\"]).columns\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    # Downcast integers\n",
    "    int_cols = df.select_dtypes(include=[\"int64\"]).columns\n",
    "    for col in int_cols:\n",
    "        # np.iinfo provides the min/max values for integer types\n",
    "        if (\n",
    "            df[col].min() >= np.iinfo(np.int8).min\n",
    "            and df[col].max() <= np.iinfo(np.int8).max\n",
    "        ):\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "        elif (\n",
    "            df[col].min() >= np.iinfo(np.int16).min\n",
    "            and df[col].max() <= np.iinfo(np.int16).max\n",
    "        ):\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "        elif (\n",
    "            df[col].min() >= np.iinfo(np.int32).min\n",
    "            and df[col].max() <= np.iinfo(np.int32).max\n",
    "        ):\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_data_for_final_fe(train_df, valid_df, test_df):\n",
    "    \"\"\"\n",
    "    A preparatory cleaning function to run before the main feature engineering.\n",
    "    1. Removes empty columns.\n",
    "    2. Converts datetimes.\n",
    "    3. Downcasts numerical dtypes to save memory.\n",
    "    4. Converts string categoricals to the memory-efficient 'category' dtype WITHOUT label encoding.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Step 1: Removing 100% Empty Features ---\")\n",
    "\n",
    "    # Identify and drop columns that are completely null in the training data\n",
    "    uniques_per_col = train_df.nunique(dropna=True)\n",
    "    empty_cols = uniques_per_col[uniques_per_col == 0].index.tolist()\n",
    "\n",
    "    if empty_cols:\n",
    "        print(f\"  -> Found {len(empty_cols)} empty columns to drop: {empty_cols}\")\n",
    "        for df in [train_df, valid_df, test_df]:\n",
    "            df.drop(columns=empty_cols, inplace=True)\n",
    "    else:\n",
    "        print(\"  -> No completely empty columns found in the training set.\")\n",
    "\n",
    "    # --- Step 2: Apply safe type conversions ---\n",
    "    print(\"\\n--- Step 2: Applying Safe Type Conversions ---\")\n",
    "\n",
    "    for df in tqdm([train_df, valid_df, test_df], desc=\"Processing DataFrames\"):\n",
    "        # Convert datetime columns\n",
    "        for col in [\"id4\", \"id5\"]:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "        # Optimize memory usage by downcasting numerical columns\n",
    "        df = downcast_numerical_dtypes(df)\n",
    "\n",
    "        # Convert high-cardinality string columns to 'category' dtype for memory efficiency.\n",
    "        # CRITICAL: We do NOT apply .cat.codes here. This preserves the original string values\n",
    "        # for the next feature engineering step, while still saving memory.\n",
    "        print(\"   -> Converting string columns to memory-efficient 'category' dtype...\")\n",
    "        object_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "        for col in object_cols:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    print(\"\\n✅ Preparatory cleaning and type casting complete.\")\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "\n",
    "def run_preparatory_preprocessing():\n",
    "    \"\"\"\n",
    "    Main execution block for this preparatory step.\n",
    "    \"\"\"\n",
    "    print(\"\\n** Running Preparatory Preprocessing Script **\")\n",
    "    print(\"   (To clean and optimize dtypes before final feature engineering)\")\n",
    "\n",
    "    # Load the split data from the previous step\n",
    "    train_df = pd.read_parquet(f\"{INPUT_DIR}/train_0.parquet\")\n",
    "    valid_df = pd.read_parquet(f\"{INPUT_DIR}/valid_0.parquet\")\n",
    "    test_df = pd.read_parquet(f\"{INPUT_DIR}/test_0.parquet\")\n",
    "\n",
    "    # Run the preparatory function\n",
    "    processed_train, processed_valid, processed_test = prepare_data_for_final_fe(\n",
    "        train_df, valid_df, test_df\n",
    "    )\n",
    "\n",
    "    # Save the prepared files, ready for the Final FE script\n",
    "    # These files have the same features, just cleaner and more memory-efficient.\n",
    "    processed_train.to_parquet(f\"{OUTPUT_DIR}/train_1_prepared.parquet\")\n",
    "    processed_valid.to_parquet(f\"{OUTPUT_DIR}/valid_1_prepared.parquet\")\n",
    "    processed_test.to_parquet(f\"{OUTPUT_DIR}/test_1_prepared.parquet\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nSaved prepared data to '{OUTPUT_DIR}/train_1_prepared.parquet' and corresponding files.\"\n",
    "    )\n",
    "\n",
    "    del train_df, valid_df, test_df, processed_train, processed_valid, processed_test\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_preparatory_preprocessing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d69796-0dce-4ab0-bdf5-2e8f13806c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import warnings\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import gc\n",
    "# import shutil\n",
    "# import uuid\n",
    "# import traceback\n",
    "# from multiprocessing import Pool, cpu_count\n",
    "# from functools import partial\n",
    "# from tqdm import tqdm\n",
    "# import pyarrow.parquet as pq\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.dataset as ds\n",
    "# from scipy.fft import rfft\n",
    "# from scipy.stats import entropy\n",
    "# import scipy.stats\n",
    "\n",
    "# # --- Configuration ---\n",
    "# warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "# warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# # --- Script Settings ---\n",
    "# SKIP_TRAINING = False\n",
    "# DATA_DICT_PATH = \"data_dictionary.csv\"\n",
    "# INPUT_DIR = \"inter\"\n",
    "# OUTPUT_DIR = \"inter\"\n",
    "# N_CORES = cpu_count()\n",
    "\n",
    "# # --- Temporary Directories for Parallel Processing ---\n",
    "# TEMP_SPLIT_DIR = os.path.join(OUTPUT_DIR, \"temp_splits\")\n",
    "# TEMP_PARTS_DIR = os.path.join(OUTPUT_DIR, \"temp_parts\")\n",
    "# # NOTE: The pre-merging step and its directory have been removed.\n",
    "\n",
    "# # --- Helper Functions ---\n",
    "# def downcast_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Downcasts float64 columns to the more memory-efficient float32 type.\n",
    "#     \"\"\"\n",
    "#     for col in df.select_dtypes(include=['float64']).columns:\n",
    "#         df[col] = df[col].astype(np.float32)\n",
    "#     return df\n",
    "\n",
    "# # --- Worker for Phase 1 (Copy-on-Write) ---\n",
    "# def split_customer_data_worker_global(customer_id_chunk, output_dir):\n",
    "#     \"\"\"\n",
    "#     Takes a chunk of customer IDs, filters the global main DataFrame for those customers,\n",
    "#     and saves each customer's data to a separate file.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         sub_df = main_df[main_df['id2'].isin(customer_id_chunk)]\n",
    "#         for customer_id, group_df in sub_df.groupby('id2'):\n",
    "#             output_file = os.path.join(output_dir, f\"customer_{customer_id}.parquet\")\n",
    "#             group_df.to_parquet(output_file)\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in splitting worker for chunk starting with {customer_id_chunk[0]}: {e}\")\n",
    "#         return False\n",
    "\n",
    "# # --- Worker for Phase 2 - Parallel Feature Engineering ---\n",
    "# def process_customer_file(customer_id, input_dir, output_dir, numerical_cols):\n",
    "#     \"\"\"\n",
    "#     Processes a SINGLE customer's data by reading it from a pre-split file,\n",
    "#     calculates complex sequential features, and saves the result.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         input_path = os.path.join(input_dir, f\"customer_{customer_id}.parquet\")\n",
    "#         group_df = pd.read_parquet(input_path)\n",
    "\n",
    "#         def clean_mixed_type(x):\n",
    "#             if isinstance(x, (list, tuple, np.ndarray)):\n",
    "#                 return x[0] if len(x) > 0 else np.nan\n",
    "#             return x\n",
    "\n",
    "#         for col in group_df.select_dtypes(include=['object']).columns:\n",
    "#             group_df[col] = group_df[col].apply(clean_mixed_type)\n",
    "\n",
    "#         f_cols = [c for c in group_df.columns if c.startswith('f')]\n",
    "#         if f_cols:\n",
    "#             group_df[f_cols] = group_df[f_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "#         group_df = group_df.copy()\n",
    "\n",
    "#         def wma(series, period):\n",
    "#             return series.ewm(alpha=2 / (period + 1), adjust=False).mean()\n",
    "\n",
    "#         def hma(series, period):\n",
    "#             if period <= 1: return series\n",
    "#             period_sqrt = max(1, int(np.sqrt(period)))\n",
    "#             wma1 = 2 * wma(series, period // 2)\n",
    "#             wma2 = wma(series, period)\n",
    "#             return wma(wma1 - wma2, period_sqrt)\n",
    "\n",
    "#         def safe_shannon_entropy(series):\n",
    "#             if len(series) < 15: return np.nan\n",
    "#             interpolated_series = series.interpolate(method='linear').ffill().bfill()\n",
    "#             if interpolated_series.isnull().any() or interpolated_series.nunique() < 2: return np.nan\n",
    "#             psd = np.abs(rfft(interpolated_series.values))**2\n",
    "#             if psd.sum() == 0: return np.nan\n",
    "#             psd_prob = psd / psd.sum()\n",
    "#             return scipy.stats.entropy(psd_prob)\n",
    "\n",
    "#         days_since_last_event = (group_df['id4'].diff().dt.total_seconds() / (3600 * 24)) + 1e-6\n",
    "#         days_since_last_event = pd.to_numeric(days_since_last_event, errors='coerce').fillna(1e-6)\n",
    "\n",
    "#         is_click = (group_df['f29'].diff() > 0) if 'f29' in group_df.columns else pd.Series([False] * len(group_df), index=group_df.index)\n",
    "#         group_df['is_click_int'] = is_click.astype(int)\n",
    "\n",
    "#         group_df['offer_interaction_count'] = group_df.groupby('id3').cumcount()\n",
    "#         if 'offer_category' in group_df.columns:\n",
    "#             group_df['category_interaction_count'] = group_df.groupby('offer_category').cumcount()\n",
    "#             category_clicks = group_df.groupby('offer_category')['is_click_int'].cumsum().shift(1).fillna(0)\n",
    "#             category_views = group_df.groupby('offer_category').cumcount()\n",
    "#             group_df['customer_category_ctr'] = category_clicks / (category_views + 1)\n",
    "\n",
    "#         if 'session_id' in group_df.columns:\n",
    "#             session_gb = group_df.groupby('session_id')\n",
    "#             group_df['session_offer_count'] = session_gb.cumcount()\n",
    "#             group_df['session_clicks'] = session_gb['is_click_int'].cumsum().shift(1).fillna(0)\n",
    "#             group_df['time_since_session_start_mins'] = (group_df['id4'] - session_gb['id4'].transform('min')).dt.total_seconds() / 60\n",
    "\n",
    "#         if 'offer_age_days' in group_df.columns:\n",
    "#             group_df['offer_age_days'] = pd.to_numeric(group_df['offer_age_days'], errors='coerce')\n",
    "#             rolling_avg_age = group_df['offer_age_days'].rolling(10, min_periods=1).mean()\n",
    "#             group_df['age_vs_recent_avg'] = group_df['offer_age_days'] - rolling_avg_age\n",
    "\n",
    "#         group_df['time_since_last_event_hours'] = days_since_last_event * 24\n",
    "#         if 'f29' in group_df.columns:\n",
    "#             click_timestamps = group_df['id4'].where(is_click).ffill()\n",
    "#             group_df['time_since_last_click'] = (group_df['id4'] - click_timestamps).dt.total_seconds() / 3600\n",
    "\n",
    "#         key_velocity_cols = {'f43': 'balance', 'f77': 'engagement_ratio', 'f59': 'time_spent'}\n",
    "#         for col, name in key_velocity_cols.items():\n",
    "#             if col in group_df.columns:\n",
    "#                 s_numeric = group_df[col]\n",
    "#                 if s_numeric.isnull().all():\n",
    "#                     group_df[f'{name}_velocity'] = np.nan\n",
    "#                     continue\n",
    "#                 change = s_numeric.diff()\n",
    "#                 velocity = change / days_since_last_event\n",
    "#                 lower_q = velocity.expanding(min_periods=5).quantile(0.01).ffill()\n",
    "#                 upper_q = velocity.expanding(min_periods=5).quantile(0.99).ffill()\n",
    "#                 group_df[f'{name}_velocity'] = velocity.clip(lower_q, upper_q)\n",
    "\n",
    "#         for col in numerical_cols:\n",
    "#             if col in group_df.columns:\n",
    "#                 s_numeric = pd.to_numeric(group_df[col], errors='coerce')\n",
    "#                 if s_numeric.isnull().all(): continue\n",
    "#                 group_df[f'{col}_diff1'] = s_numeric.diff(1)\n",
    "#                 for k in [5, 10, 20]:\n",
    "#                     group_df[f'{col}hma{k}'] = hma(s_numeric, k)\n",
    "#                 for k in [10, 20]:\n",
    "#                     group_df[f'{col}roll_std{k}'] = s_numeric.rolling(k, min_periods=3).std()\n",
    "\n",
    "#         key_ts_cols = ['f218', 'f219', 'f220', 'f102']\n",
    "#         for col in key_ts_cols:\n",
    "#             if col in group_df.columns:\n",
    "#                 s_numeric = pd.to_numeric(group_df[col], errors='coerce')\n",
    "#                 if s_numeric.isnull().all(): continue\n",
    "#                 rm = s_numeric.rolling(5, min_periods=1).mean()\n",
    "#                 rv = s_numeric.rolling(5, min_periods=1).var()\n",
    "#                 group_df[f'{col}_stability_5'] = rm.rolling(5, min_periods=1).var()\n",
    "#                 group_df[f'{col}_lumpiness_5'] = rv.rolling(5, min_periods=1).var()\n",
    "#                 group_df[f'{col}_rolling_shannon_entropy'] = s_numeric.rolling(20, min_periods=15).apply(safe_shannon_entropy, raw=False)\n",
    "\n",
    "#         group_df['time_since_last_seen_this_offer'] = group_df.groupby('id3')['id4'].diff().dt.total_seconds() / 3600\n",
    "#         if 'offer_category' in group_df.columns:\n",
    "#             group_df['time_since_last_seen_this_category'] = group_df.groupby('offer_category')['id4'].diff().dt.total_seconds() / 3600\n",
    "\n",
    "#         group_df.drop(columns=['is_click_int', 'session_id'], inplace=True, errors='ignore')\n",
    "#         group_df = downcast_dtypes(group_df)\n",
    "#         output_path = os.path.join(output_dir, f\"part_{customer_id}.parquet\")\n",
    "#         group_df.to_parquet(output_path)\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n--- CRITICAL ERROR: Processing failed for customer ID: {customer_id} ---\")\n",
    "#         traceback.print_exc()\n",
    "#         return False\n",
    "\n",
    "# # --- NEW: OOM-Safe Merge using PyArrow Dataset API ---\n",
    "# def merge_with_dataset_api(source_dir, output_path, schema, is_test_run=False):\n",
    "#     \"\"\"\n",
    "#     Merges all Parquet files in a directory into a single file using the\n",
    "#     memory-efficient PyArrow Dataset API.\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nStep 4/5: Merging files from '{source_dir}' into '{output_path}' using Dataset API...\")\n",
    "    \n",
    "#     # Define the columns to write, excluding 'y' for test runs\n",
    "#     columns_to_write = schema.names\n",
    "#     if is_test_run and 'y' in columns_to_write:\n",
    "#         print(\" -> Test run detected. Excluding 'y' column from final output.\")\n",
    "#         columns_to_write.remove('y')\n",
    "\n",
    "#     # Create a dataset from the directory of part files\n",
    "#     dataset = ds.dataset(source_dir, format=\"parquet\", schema=schema)\n",
    "    \n",
    "#     # Create a scanner to select columns. This is a lazy operation.\n",
    "#     scanner = dataset.scanner(columns=columns_to_write)\n",
    "    \n",
    "#     # Use a temporary file for writing to avoid conflicts if the script is re-run\n",
    "#     temp_output_file = output_path + \".tmp\"\n",
    "\n",
    "#     # Write the dataset to a single new Parquet file\n",
    "#     ds.write_dataset(\n",
    "#         scanner,\n",
    "#         base_dir=os.path.dirname(temp_output_file),\n",
    "#         basename_template=os.path.basename(temp_output_file),\n",
    "#         format=\"parquet\",\n",
    "#         max_partitions=1, # Force writing to a single file\n",
    "#         existing_data_behavior='overwrite_or_ignore'\n",
    "#     )\n",
    "    \n",
    "#     # Rename the temporary file to the final output path\n",
    "#     if os.path.exists(output_path):\n",
    "#         os.remove(output_path)\n",
    "#     os.rename(temp_output_file, output_path)\n",
    "\n",
    "#     print(f\" -> Successfully merged files.\")\n",
    "\n",
    "# # --- Main Feature Engineering Function ---\n",
    "# def engineer_features_final(\n",
    "#     df: pd.DataFrame,\n",
    "#     output_path: str,\n",
    "#     data_dict_path: str,\n",
    "#     global_start_time=None,\n",
    "#     offer_profile_features=None,\n",
    "#     master_schema=None,\n",
    "#     is_test_run=False\n",
    "# ):\n",
    "#     print(f\"\\nProcessing DataFrame with shape: {df.shape} to create file: {output_path}\")\n",
    "#     print(f\"Using {N_CORES} cores for parallel steps.\")\n",
    "\n",
    "#     for temp_dir in [TEMP_SPLIT_DIR, TEMP_PARTS_DIR]:\n",
    "#         if os.path.exists(temp_dir): shutil.rmtree(temp_dir)\n",
    "#         os.makedirs(temp_dir)\n",
    "#         print(f\"  -> Created temporary directory: {temp_dir}\")\n",
    "\n",
    "#     data_dict = pd.read_csv(data_dict_path)\n",
    "#     type_map = {row[\"masked_column\"]: row[\"Type\"].strip() for _, row in data_dict.iterrows()}\n",
    "#     numerical_cols = [col for col in df.columns if col.startswith(\"f\") and type_map.get(col) == \"Numerical\"]\n",
    "\n",
    "#     print(\"Step 1/5: Engineering fast, vectorized features...\")\n",
    "#     df[\"id4\"] = pd.to_datetime(df[\"id4\"])\n",
    "#     df = df.sort_values(by=['id2', 'id4'])\n",
    "#     time_diff_mins = df.groupby('id2')['id4'].diff().dt.total_seconds().div(60)\n",
    "#     session_break = (time_diff_mins > 30).cumsum()\n",
    "#     df['session_id'] = df['id2'].astype(str) + '_' + session_break.astype(str)\n",
    "#     df.rename(columns={'f223': 'offer_age_days', 'f224': 'offer_time_to_expiry_days'}, inplace=True)\n",
    "\n",
    "#     df['customer_account_age_days'] = (df['id4'] - df.groupby('id2')['id4'].transform('min')).dt.total_seconds() / (3600 * 24)\n",
    "#     if global_start_time is None: global_start_time = df['id4'].min()\n",
    "#     df['time_since_dataset_start_days'] = (df['id4'] - global_start_time).dt.total_seconds() / (3600 * 24)\n",
    "\n",
    "#     category_cols = [f'f{i}' for i in range(226, 233)]; existing_cat_cols = [c for c in category_cols if c in df.columns]\n",
    "#     if existing_cat_cols:\n",
    "#         df[existing_cat_cols] = df[existing_cat_cols].apply(pd.to_numeric, errors='coerce')\n",
    "#         df['offer_category'] = df[existing_cat_cols].idxmax(axis=1)\n",
    "\n",
    "#     if offer_profile_features is None:\n",
    "#         print(\"Step 2/5: Computing offer profiles from training data...\")\n",
    "#         base_offer_profiles = df.groupby('id3').agg(\n",
    "#             offer_popularity=('id1', 'count'),\n",
    "#             offer_customer_reach=('id2', 'nunique')\n",
    "#         )\n",
    "#         monthly_counts = df.groupby([df['id4'].dt.to_period('M'), 'id3'])['id1'].count().unstack('id3').fillna(0)\n",
    "        \n",
    "#         if len(monthly_counts) >= 6:\n",
    "#             growth_rate = (monthly_counts.iloc[-3:].mean() - monthly_counts.iloc[-6:-3].mean()) / (monthly_counts.iloc[-6:-3].mean() + 1e-6)\n",
    "#             base_offer_profiles['offer_growth_rate'] = growth_rate\n",
    "#         else:\n",
    "#             base_offer_profiles['offer_growth_rate'] = np.nan\n",
    "            \n",
    "#         base_offer_profiles['offer_lifecycle_stage'] = pd.cut(base_offer_profiles['offer_growth_rate'], bins=[-np.inf, 0, 0.2, np.inf], labels=[0, 1, 2]).astype('Int8')\n",
    "#         offer_profile_features = base_offer_profiles\n",
    "#     else:\n",
    "#         print(\"Step 2/5: Using pre-computed offer profiles for validation/test data.\")\n",
    "\n",
    "#     print(\"Step 2.5/5: Engineering leakage-free historical features...\")\n",
    "#     for col in tqdm(offer_profile_features.columns, desc=\"    -> Mapping offer profiles\"):\n",
    "#         df[col] = df['id3'].map(offer_profile_features[col])\n",
    "\n",
    "#     if 'y' in df.columns:\n",
    "#         df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "\n",
    "#     if not is_test_run and 'y' in df.columns:\n",
    "#         offer_group = df.groupby('id3')\n",
    "#         historical_clicks = offer_group['y'].cumsum().shift(1)\n",
    "#         historical_views = offer_group.cumcount()\n",
    "#         df['offer_historical_ctr_fixed'] = (historical_clicks / (historical_views + 1)).fillna(0)\n",
    "#     else:\n",
    "#         df['offer_historical_ctr_fixed'] = 0\n",
    "\n",
    "#     cust_popularity_avg = df.groupby('id2')['offer_popularity'].expanding().mean().shift(1)\n",
    "#     cust_popularity_avg = cust_popularity_avg.reset_index(level=0, drop=True)\n",
    "#     df['popularity_vs_customer_norm'] = df['offer_popularity'] - cust_popularity_avg.fillna(df['offer_popularity'])\n",
    "\n",
    "#     print(\"Step 2.7/5: Engineering dynamic customer profiles...\")\n",
    "#     all_profile_cols = numerical_cols\n",
    "#     for col in tqdm(all_profile_cols, desc=\"    -> Calculating dynamic profiles\"):\n",
    "#         if col in df.columns:\n",
    "#             customer_group = df.groupby('id2')[col]\n",
    "#             expanding_mean = customer_group.expanding().mean().shift(1)\n",
    "#             expanding_std = customer_group.expanding().std().shift(1)\n",
    "#             expanding_max = customer_group.expanding().max().shift(1)\n",
    "#             expanding_min = customer_group.expanding().min().shift(1)\n",
    "#             df[f'dynamic_{col}_mean'] = expanding_mean.reset_index(level=0, drop=True)\n",
    "#             df[f'dynamic_{col}_std'] = expanding_std.reset_index(level=0, drop=True)\n",
    "#             df[f'dynamic_{col}_max'] = expanding_max.reset_index(level=0, drop=True)\n",
    "#             df[f'dynamic_{col}_min'] = expanding_min.reset_index(level=0, drop=True)\n",
    "\n",
    "#     fill_cols = [c for c in df.columns if c.startswith('dynamic_')]\n",
    "#     df[fill_cols] = df[fill_cols].fillna(0)\n",
    "\n",
    "#     print(f\"\\nStep 3/5: Pre-splitting data for {df['id2'].nunique()} customers in parallel...\")\n",
    "#     global main_df\n",
    "#     main_df = df\n",
    "#     customer_ids = main_df['id2'].unique()\n",
    "#     id_chunks = np.array_split(customer_ids, N_CORES * 8)\n",
    "#     split_func = partial(split_customer_data_worker_global, output_dir=TEMP_SPLIT_DIR)\n",
    "#     with Pool(N_CORES) as p:\n",
    "#         for _ in tqdm(p.imap_unordered(split_func, id_chunks), total=len(id_chunks), desc=\" -> Parallel Splitting\"):\n",
    "#             pass\n",
    "#     del df, main_df; gc.collect()\n",
    "\n",
    "#     print(f\"Step 3.5/5: Discovering and processing customer files...\")\n",
    "#     customer_files = [f for f in os.listdir(TEMP_SPLIT_DIR) if f.startswith('customer_') and f.endswith('.parquet')]\n",
    "#     customer_ids_from_files = [f.split('_')[1].split('.')[0] for f in customer_files]\n",
    "#     print(f\"    -> Found {len(customer_ids_from_files)} customer files to process.\")\n",
    "\n",
    "#     if customer_ids_from_files:\n",
    "#         processing_func = partial(process_customer_file, input_dir=TEMP_SPLIT_DIR, output_dir=TEMP_PARTS_DIR, numerical_cols=numerical_cols)\n",
    "#         with Pool(N_CORES) as p:\n",
    "#             for _ in tqdm(p.imap_unordered(processing_func, customer_ids_from_files), total=len(customer_ids_from_files), desc=\" -> Feature Engineering\"):\n",
    "#                 pass\n",
    "#     shutil.rmtree(TEMP_SPLIT_DIR)\n",
    "\n",
    "#     part_files = [os.path.join(TEMP_PARTS_DIR, f) for f in os.listdir(TEMP_PARTS_DIR) if f.endswith('.parquet')]\n",
    "#     if not part_files:\n",
    "#         print(\"Warning: No temporary part files were created.\")\n",
    "#         shutil.rmtree(TEMP_PARTS_DIR)\n",
    "#         return offer_profile_features, None, global_start_time\n",
    "\n",
    "#     if master_schema is None:\n",
    "#         print(\"     -> No master schema found. Creating a robust union schema from a sample of part files...\")\n",
    "#         sample_files = part_files[:min(200, len(part_files))]\n",
    "#         all_schemas = [pq.read_schema(f) for f in sample_files]\n",
    "#         all_fields = {}\n",
    "#         for schema in all_schemas:\n",
    "#             for field in schema:\n",
    "#                 if field.name not in all_fields or pa.types.is_null(all_fields[field.name].type):\n",
    "#                     all_fields[field.name] = field\n",
    "#         master_schema = pa.schema(list(all_fields.values()))\n",
    "#         print(\"     -> Master schema created successfully.\")\n",
    "\n",
    "#     # --- REVISED STEP 4 & 5: OOM-Safe Merge using PyArrow Dataset API ---\n",
    "#     merge_with_dataset_api(TEMP_PARTS_DIR, output_path, master_schema, is_test_run)\n",
    "#     shutil.rmtree(TEMP_PARTS_DIR) # Clean up the thousands of small files\n",
    "\n",
    "#     print(f\"✅ Finished creating final file: {output_path}\")\n",
    "#     return offer_profile_features, master_schema, global_start_time\n",
    "\n",
    "# # --- Main Execution ---\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"🚀 *** Optimized Production-Grade Feature Engineering Script ***\")\n",
    "#     os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "#     offer_profiles_artifact = None\n",
    "#     master_schema_artifact = None\n",
    "#     global_start_time_artifact = None\n",
    "\n",
    "#     if not SKIP_TRAINING:\n",
    "#         print(\"\\n--- Processing Training Data ---\")\n",
    "#         train_df = pd.read_parquet(f\"{INPUT_DIR}/train_1_prepared.parquet\", engine='fastparquet')\n",
    "\n",
    "#         offer_profiles_artifact, master_schema_artifact, global_start_time_artifact = engineer_features_final(\n",
    "#             train_df,\n",
    "#             output_path=f\"{OUTPUT_DIR}/train_2_final.parquet\",\n",
    "#             data_dict_path=DATA_DICT_PATH\n",
    "#         )\n",
    "#         offer_profiles_artifact.to_parquet(f\"{OUTPUT_DIR}/offer_profiles_artifact.parquet\")\n",
    "\n",
    "#         if master_schema_artifact:\n",
    "#             with open(f\"{OUTPUT_DIR}/master_schema.arrow\", \"wb\") as f:\n",
    "#                 serialized_buffer = master_schema_artifact.serialize()\n",
    "#                 f.write(serialized_buffer.to_pybytes())\n",
    "\n",
    "#         del train_df; gc.collect()\n",
    "#     else:\n",
    "#         print(\"\\n--- Skipping Training Phase: Loading artifacts from disk... ---\")\n",
    "#         offer_profiles_artifact = pd.read_parquet(f\"{OUTPUT_DIR}/offer_profiles_artifact.parquet\", engine='fastparquet')\n",
    "#         with open(f\"{OUTPUT_DIR}/master_schema.arrow\", \"rb\") as f:\n",
    "#             master_schema_artifact = pa.ipc.read_schema(pa.py_buffer(f.read()))\n",
    "\n",
    "#         train_pq_file = pq.ParquetFile(f\"{OUTPUT_DIR}/train_2_final.parquet\")\n",
    "#         global_start_time_artifact = train_pq_file.read(columns=['id4'])['id4'].min().as_py()\n",
    "#         print(\" -> Artifacts loaded successfully.\")\n",
    "\n",
    "#     print(\"\\n--- Processing Validation Data ---\")\n",
    "#     valid_df = pd.read_parquet(f\"{INPUT_DIR}/valid_1_prepared.parquet\", engine='fastparquet')\n",
    "#     valid_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     _, _, _ = engineer_features_final(\n",
    "#         valid_df,\n",
    "#         output_path=f\"{OUTPUT_DIR}/valid_2_final.parquet\",\n",
    "#         data_dict_path=DATA_DICT_PATH,\n",
    "#         global_start_time=global_start_time_artifact,\n",
    "#         offer_profile_features=offer_profile_features,\n",
    "#         master_schema=master_schema_artifact\n",
    "#     )\n",
    "#     del valid_df; gc.collect()\n",
    "\n",
    "#     print(\"\\n--- Processing Test Data ---\")\n",
    "#     test_file_path = f\"{INPUT_DIR}/test_1_prepared.parquet\"\n",
    "#     if os.path.exists(test_file_path):\n",
    "#         test_df = pd.read_parquet(test_file_path, engine='fastparquet')\n",
    "#         test_df.reset_index(drop=True, inplace=True)\n",
    "#         if 'y' not in test_df.columns: test_df['y'] = -1\n",
    "\n",
    "#         _, _, _ = engineer_features_final(\n",
    "#             test_df,\n",
    "#             output_path=f\"{OUTPUT_DIR}/test_2_final.parquet\",\n",
    "#             data_dict_path=DATA_DICT_PATH,\n",
    "#             global_start_time=global_start_time_artifact,\n",
    "#             offer_profile_features=offer_profiles_artifact,\n",
    "#             master_schema=master_schema_artifact,\n",
    "#             is_test_run=True\n",
    "#         )\n",
    "#         del test_df, offer_profiles_artifact\n",
    "#         gc.collect()\n",
    "#     else:\n",
    "#         print(f\"\\n'{test_file_path}' not found. Skipping test set processing.\")\n",
    "\n",
    "#     print(\"\\n--- All datasets have been feature-engineered and saved successfully. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b007a-22e2-40f7-b27c-4cd7b37a263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import gc\n",
    "\n",
    "# print(\"🚀 --- Starting Data Reconciliation Script --- 🚀\")\n",
    "\n",
    "# # --- Configuration ---\n",
    "# # Directory where your intermediate files are stored\n",
    "# INPUT_DIR = \"inter\" \n",
    "\n",
    "# # Base names for the file sets to process\n",
    "# DATA_SETS = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "# for dataset_name in DATA_SETS:\n",
    "#     print(f\"\\n--- Processing: {dataset_name} ---\")\n",
    "\n",
    "#     # --- File Paths ---\n",
    "#     # Path to the clean base file from your new pipeline\n",
    "#     prepared_path = os.path.join(INPUT_DIR, f\"{dataset_name}_1_prepared.parquet\")\n",
    "#     # Path to the feature-rich file from your OLD pipeline's output\n",
    "#     final_old_path = os.path.join(INPUT_DIR, f\"{dataset_name}_2_final.parquet\")\n",
    "#     # Path for the new, reconciled output file. This matches the expected input for the next step.\n",
    "#     output_path = os.path.join(INPUT_DIR, f\"{dataset_name}_2_final.parquet\")\n",
    "\n",
    "#     if not os.path.exists(prepared_path) or not os.path.exists(final_old_path):\n",
    "#         print(f\"  -> ⚠  Skipping {dataset_name}: One or both input files not found.\")\n",
    "#         continue\n",
    "\n",
    "#     # --- Load DataFrames ---\n",
    "#     print(f\"  -> Loading '{os.path.basename(prepared_path)}'...\")\n",
    "#     df_prepared = pd.read_parquet(prepared_path)\n",
    "    \n",
    "#     print(f\"  -> Loading '{os.path.basename(final_old_path)}' (from old pipeline)...\")\n",
    "#     df_final_old = pd.read_parquet(final_old_path)\n",
    "\n",
    "#     print(f\"  -> Original shapes: Prepared={df_prepared.shape}, Old Final={df_final_old.shape}\")\n",
    "\n",
    "#     # --- Identify New Columns ---\n",
    "#     # Find which columns were created in the old advanced feature engineering step.\n",
    "#     # These are the columns present in the old final file but NOT in the prepared file.\n",
    "#     prepared_cols = set(df_prepared.columns)\n",
    "#     final_old_cols = set(df_final_old.columns)\n",
    "#     new_feature_cols = list(final_old_cols - prepared_cols)\n",
    "    \n",
    "#     print(f\"  -> Identified {len(new_feature_cols)} new features to merge.\")\n",
    "\n",
    "#     # We need the primary key 'id1' to perform the merge\n",
    "#     merge_cols = ['id1'] + new_feature_cols\n",
    "    \n",
    "#     # --- Perform the Merge ---\n",
    "#     # We use a left merge to ensure we keep all rows from the clean prepared dataframe\n",
    "#     # and add the corresponding new features.\n",
    "#     print(\"  -> Merging new features onto the prepared base dataframe...\")\n",
    "#     reconciled_df = pd.merge(\n",
    "#         df_prepared,\n",
    "#         df_final_old[merge_cols],\n",
    "#         on='id1',\n",
    "#         how='left'\n",
    "#     )\n",
    "\n",
    "#     print(f\"  -> Reconciled shape: {reconciled_df.shape}\")\n",
    "\n",
    "#     # --- Save the Reconciled DataFrame ---\n",
    "#     # This overwrites the old _2_final file with our new, corrected version.\n",
    "#     print(f\"  -> Saving reconciled data to '{os.path.basename(output_path)}'...\")\n",
    "#     reconciled_df.to_parquet(output_path, index=False)\n",
    "    \n",
    "#     # --- Clean up memory ---\n",
    "#     del df_prepared, df_final_old, reconciled_df\n",
    "#     gc.collect()\n",
    "\n",
    "# print(\"\\n✅ --- Reconciliation Complete --- ✅\")\n",
    "# print(\"You can now SKIP the 'Optimized Production-Grade Feature Engineering' cell and run the subsequent cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "INPUT_DIR = \"inter\"\n",
    "OUTPUT_DIR = \"inter\"\n",
    "\n",
    "# --- RE-INTRODUCED: Using your hardcoded list ---\n",
    "ALL_CATEGORICAL_FEATURES = [\n",
    "    \"brand_id\", \"industry_id\", \"member_industry_code\", \n",
    "    \"offer_start_dayofweek\", \"is_industry_match\", \"has_keyword_cashback\", \n",
    "    \"has_keyword_points\", \"has_keyword_discount\", \"has_keyword_spend_x\", \n",
    "    \"offer_category\", \"time_of_day_bin\", \"offer_lifecycle_stage\", \n",
    "    \"is_weekend\", \"is_holiday_week\", \"is_payday_week\"\n",
    "]\n",
    "\n",
    "# --- RE-INTRODUCED: Your memory downcasting function ---\n",
    "def downcast_numerical_dtypes(df):\n",
    "    \"\"\"\n",
    "    Intelligently downcasts numerical columns to the smallest possible format\n",
    "    to save memory.\n",
    "    \"\"\"\n",
    "    # Downcast floats\n",
    "    float_cols = df.select_dtypes(include=[\"float64\"]).columns\n",
    "    for col in float_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').astype(np.float32)\n",
    "\n",
    "    # Downcast integers\n",
    "    int_cols = df.select_dtypes(include=[\"int64\"]).columns\n",
    "    for col in int_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        if df[col].isnull().any():\n",
    "            continue\n",
    "        min_val, max_val = df[col].min(), df[col].max()\n",
    "        if min_val >= np.iinfo(np.int8).min and max_val <= np.iinfo(np.int8).max:\n",
    "            df[col] = df[col].astype(np.int8)\n",
    "        elif min_val >= np.iinfo(np.int16).min and max_val <= np.iinfo(np.int16).max:\n",
    "            df[col] = df[col].astype(np.int16)\n",
    "        elif min_val >= np.iinfo(np.int32).min and max_val <= np.iinfo(np.int32).max:\n",
    "            df[col] = df[col].astype(np.int32)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocess_for_model(train_df, valid_df, test_df):\n",
    "    \"\"\"\n",
    "    A robust, final preprocessing function using a hybrid approach.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Final Preprocessing for Model Training ---\")\n",
    "\n",
    "    print(\"Step 1/5: Removing completely empty columns...\")\n",
    "    uniques_per_col = train_df.nunique(dropna=True)\n",
    "    empty_cols = uniques_per_col[uniques_per_col == 0].index.tolist()\n",
    "    if empty_cols:\n",
    "        print(f\"  -> Found {len(empty_cols)} empty columns to drop: {empty_cols}\")\n",
    "        for df in [train_df, valid_df, test_df]:\n",
    "            df.drop(columns=empty_cols, inplace=True, errors='ignore')\n",
    "\n",
    "    print(f\"\\nStep 2/5: Applying Label Encoding to {len(ALL_CATEGORICAL_FEATURES)} specified categorical features...\")\n",
    "    for col in tqdm(ALL_CATEGORICAL_FEATURES, desc=\"Encoding Specified Cats\"):\n",
    "        if col in train_df.columns:\n",
    "            train_df[col], valid_df[col], test_df[col] = train_df[col].fillna(\"Missing\"), valid_df[col].fillna(\"Missing\"), test_df[col].fillna(\"Missing\")\n",
    "            learned_categories = train_df[col].astype('category').dtype\n",
    "            for df in [train_df, valid_df, test_df]:\n",
    "                df[col] = df[col].astype(learned_categories).cat.codes.astype('int16')\n",
    "\n",
    "    # --- NEW: Safety net to handle any remaining object columns ---\n",
    "    print(\"\\nStep 3/5: Catch-all for any remaining object columns...\")\n",
    "    remaining_obj_cols = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    if remaining_obj_cols:\n",
    "        print(f\"  -> Found {len(remaining_obj_cols)} leftover object columns to encode: {remaining_obj_cols}\")\n",
    "        for col in tqdm(remaining_obj_cols, desc=\"Encoding Leftover Cats\"):\n",
    "            train_df[col], valid_df[col], test_df[col] = train_df[col].fillna(\"Missing\"), valid_df[col].fillna(\"Missing\"), test_df[col].fillna(\"Missing\")\n",
    "            learned_categories = train_df[col].astype('category').dtype\n",
    "            for df in [train_df, valid_df, test_df]:\n",
    "                df[col] = df[col].astype(learned_categories).cat.codes.astype('int16')\n",
    "    else:\n",
    "        print(\"  -> No leftover object columns found. All good.\")\n",
    "\n",
    "\n",
    "    print(\"\\nStep 4/5: Finalizing data (handling infinities and downcasting)...\")\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        # Using your downcasting function\n",
    "        df = downcast_numerical_dtypes(df)\n",
    "\n",
    "    print(\"\\nStep 5/5: Finalizing label 'y' dtype...\")\n",
    "    for df in [train_df, valid_df]:\n",
    "        if 'y' in df.columns:\n",
    "            df['y'] = df['y'].astype(np.int8)\n",
    "\n",
    "    print(\"\\n✅ Final preprocessing is complete.\")\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "\n",
    "def run_final_preprocessing():\n",
    "    \"\"\"Main execution block.\"\"\"\n",
    "    print(\"\\n** Running Final Hybrid Preprocessing Script **\")\n",
    "    try:\n",
    "        train_df = pd.read_parquet(f\"{INPUT_DIR}/train_2_final.parquet\")\n",
    "        valid_df = pd.read_parquet(f\"{INPUT_DIR}/valid_2_final.parquet\")\n",
    "        test_df = pd.read_parquet(f\"{INPUT_DIR}/test_2_final.parquet\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ ERROR: Could not find input file. Details: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- FIX: Remove 'session_id' if it exists ---\n",
    "    for df in [train_df, valid_df, test_df]:\n",
    "        if 'session_id' in df.columns:\n",
    "            print(\"Removing 'session_id' column...\")\n",
    "            df.drop(columns=['session_id'], inplace=True, errors='ignore')\n",
    "            break\n",
    "\n",
    "    processed_train, processed_valid, processed_test = preprocess_for_model(\n",
    "        train_df, valid_df, test_df\n",
    "    )\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"\\nSaving model-ready data...\")\n",
    "    # Using engine='fastparquet' and index=False to prevent previous errors\n",
    "    processed_train.to_parquet(f\"{OUTPUT_DIR}/train_3_model_ready.parquet\", index=False, engine='fastparquet')\n",
    "    processed_valid.to_parquet(f\"{OUTPUT_DIR}/valid_3_model_ready.parquet\", index=False, engine='fastparquet')\n",
    "    processed_test.to_parquet(f\"{OUTPUT_DIR}/test_3_model_ready.parquet\", index=False, engine='fastparquet')\n",
    "    print(f\"  -> Saved model-ready data to '{OUTPUT_DIR}/'\")\n",
    "\n",
    "    del train_df, valid_df, test_df, processed_train, processed_valid, processed_test\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_final_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41295ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# GPU-Accelerated Feature Selection for Ranking - Refactored and Corrected Script\n",
    "# =================================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Import RAPIDS libraries for GPU acceleration\n",
    "try:\n",
    "    import cudf\n",
    "    import cupy as cp\n",
    "    import treelite\n",
    "    from cuml import ForestInference\n",
    "    print(\"✅ RAPIDS libraries imported successfully.\")\n",
    "except ImportError:\n",
    "    print(\"❌ WARNING: RAPIDS libraries (cudf, cuml, cupy) not found. GPU acceleration will not be available.\")\n",
    "    # Set placeholders if RAPIDS is not installed to avoid script crashing immediately\n",
    "    cudf = cp = treelite = ForestInference = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR = \"inter\"\n",
    "TRAIN_INPUT_PATH = f\"{INPUT_DIR}/train_3_model_ready.parquet\"\n",
    "VALID_INPUT_PATH = f\"{INPUT_DIR}/valid_3_model_ready.parquet\"\n",
    "MODEL_DIR = \"model\"\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# =================================================================================\n",
    "# === PERFORMANCE OPTIMIZATION: Two-Stage Feature Selection =======================\n",
    "# =================================================================================\n",
    "# Stage 1: Use LGBM's fast importance to find the top N candidates.\n",
    "N_CANDIDATE_FEATURES = 100\n",
    "# Stage 2: Use the slower, more accurate permutation importance to select the\n",
    "# final top N features from the candidates.\n",
    "TOP_N_FEATURES = 50\n",
    "# =================================================================================\n",
    "\n",
    "\n",
    "# --- Feature Definitions ---\n",
    "ALL_CATEGORICAL_FEATURES = [\n",
    "    \"brand_id\", \"industry_id\", \"member_industry_code\", \n",
    "    \"offer_start_dayofweek\", \"is_industry_match\", \"has_keyword_cashback\", \n",
    "    \"has_keyword_points\", \"has_keyword_discount\", \"has_keyword_spend_x\", \n",
    "    \"offer_category\", \"time_of_day_bin\", \"offer_lifecycle_stage\", \n",
    "    \"is_weekend\", \"is_holiday_week\", \"is_payday_week\"\n",
    "]\n",
    "TARGET_COL = \"y\"\n",
    "GROUP_COLS = [\"id2\", \"id5\"]\n",
    "ID_COLS_TO_EXCLUDE = [\"id1\", \"id3\", \"id4\"]\n",
    "\n",
    "\n",
    "def optimize_data_types(df):\n",
    "    \"\"\"\n",
    "    Downcasts numerical columns to more memory-efficient types.\n",
    "    \"\"\"\n",
    "    for col in df.select_dtypes('float').columns:\n",
    "        df[col] = df[col].astype('float32')\n",
    "    for col in df.select_dtypes('integer').columns:\n",
    "        # Safely downcast integers, checking min/max ranges\n",
    "        if df[col].min() >= np.iinfo('int32').min and df[col].max() <= np.iinfo('int32').max:\n",
    "            df[col] = df[col].astype('int32')\n",
    "    return df\n",
    "\n",
    "def load_data(train_path, valid_path):\n",
    "    \"\"\"\n",
    "    Loads and optimizes training and validation data.\n",
    "    \"\"\"\n",
    "    print(\"--- Loading and optimizing data ---\")\n",
    "    try:\n",
    "        train_df = pd.read_parquet(train_path, engine='fastparquet')\n",
    "        valid_df = pd.read_parquet(valid_path, engine='fastparquet')\n",
    "        \n",
    "        train_df = optimize_data_types(train_df)\n",
    "        valid_df = optimize_data_types(valid_df)\n",
    "        \n",
    "        print(f\"  -> Train data shape: {train_df.shape}\")\n",
    "        print(f\"  -> Valid data shape: {valid_df.shape}\")\n",
    "        \n",
    "        return train_df, valid_df\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ ERROR: Could not find input files. Details: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def calculate_map_at_7_gpu(y_true_gpu, y_pred_gpu, groups_df_gpu, n=7):\n",
    "    \"\"\"\n",
    "    Calculates Mean Average Precision @ 7 entirely on the GPU using cuDF.\n",
    "    \"\"\"\n",
    "    df = groups_df_gpu.copy()\n",
    "    df['y'] = y_true_gpu\n",
    "    df['pred'] = cudf.Series(y_pred_gpu)\n",
    "\n",
    "    map_denominators = df.groupby(GROUP_COLS)['y'].sum().clip(upper=n).rename('map_denominator')\n",
    "\n",
    "    df = df.sort_values(GROUP_COLS + [\"pred\"], ascending=[True, True, False])\n",
    "    df['position'] = df.groupby(GROUP_COLS).cumcount() + 1\n",
    "    \n",
    "    df_top7 = df[df['position'] <= n].copy()\n",
    "    df_top7['cumsum_y'] = df_top7.groupby(GROUP_COLS)['y'].cumsum()\n",
    "    df_top7['ap_numerator_term'] = (df_top7['cumsum_y'] / df_top7['position']) * df_top7['y']\n",
    "    ap_numerators = df_top7.groupby(GROUP_COLS)['ap_numerator_term'].sum().rename('map_numerator')\n",
    "    \n",
    "    ap_df = ap_numerators.to_frame().merge(\n",
    "        map_denominators.to_frame(),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    ).fillna(0)\n",
    "    \n",
    "    ap_df['ap_at_7'] = ap_df['map_numerator'] / ap_df['map_denominator']\n",
    "    \n",
    "    return ap_df['ap_at_7'].fillna(0).mean()\n",
    "\n",
    "def train_lgbm_ranker(X_train, y_train, train_groups, X_valid, y_valid, valid_groups, purpose=\"\"):\n",
    "    \"\"\"\n",
    "    Trains an LGBMRanker model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Training LGBMRanker for {purpose} with {X_train.shape[1]} features ---\")\n",
    "    \n",
    "    # Ensure categorical features are explicitly converted to 'category' dtype for LightGBM\n",
    "    categorical_features_for_lgbm = [col for col in X_train.columns if col in ALL_CATEGORICAL_FEATURES]\n",
    "    for col in categorical_features_for_lgbm:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].astype('category')\n",
    "        if col in X_valid.columns:\n",
    "            X_valid[col] = X_valid[col].astype('category')\n",
    "\n",
    "    temp_params = {\n",
    "        'objective': 'lambdarank', 'metric': 'map', 'eval_at': [7],\n",
    "        'boosting_type': 'gbdt', 'n_estimators': 2000, 'learning_rate': 0.03,\n",
    "        'num_leaves': 40, 'max_depth': 7, 'colsample_bytree': 0.7, 'subsample': 0.7,\n",
    "        'seed': RANDOM_STATE, 'feature_fraction_seed': RANDOM_STATE, 'data_random_seed': RANDOM_STATE,\n",
    "        'n_jobs': -1, 'verbose': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRanker(**temp_params)\n",
    "    model.fit(X_train, y_train, group=train_groups, eval_set=[(X_valid, y_valid)],\n",
    "              eval_group=[valid_groups], callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "              categorical_feature='auto') # Let LightGBM handle categoricals automatically\n",
    "    \n",
    "    return model\n",
    "\n",
    "def calculate_grouped_permutation_importance(model, X_valid, y_valid, valid_df):\n",
    "    \"\"\"\n",
    "    Calculates group-aware permutation importance on a reduced set of features.\n",
    "    \"\"\"\n",
    "    if cudf is None:\n",
    "        raise ImportError(\"RAPIDS must be installed for this function.\")\n",
    "\n",
    "    print(\"\\n--- Preparing data and model for GPU-accelerated Permutation Importance ---\")\n",
    "    model_path = os.path.join(MODEL_DIR, \"temp_ranker_for_importance.txt\")\n",
    "    model.booster_.save_model(model_path)\n",
    "    tl_model = treelite.Model.load(model_path, model_format='lightgbm')\n",
    "    fil_model = ForestInference.load_from_treelite_model(tl_model)\n",
    "\n",
    "    feature_cols = X_valid.columns.tolist()\n",
    "\n",
    "    print(\"  -> Pre-processing columns by type before GPU conversion...\")\n",
    "    numerical_cols = X_valid.select_dtypes(include=np.number).columns.tolist()\n",
    "    non_numerical_cols = X_valid.select_dtypes(exclude=np.number).columns.tolist()\n",
    "    X_valid[numerical_cols] = X_valid[numerical_cols].fillna(0)\n",
    "    for col_name in non_numerical_cols:\n",
    "        if not pd.api.types.is_categorical_dtype(X_valid[col_name]):\n",
    "            X_valid[col_name] = X_valid[col_name].astype('category')\n",
    "        if '0' not in X_valid[col_name].cat.categories:\n",
    "            X_valid[col_name] = X_valid[col_name].cat.add_categories(['0'])\n",
    "        X_valid[col_name] = X_valid[col_name].fillna('0')\n",
    "    \n",
    "    X_valid_gpu = cudf.from_pandas(X_valid)\n",
    "    \n",
    "    print(\"  -> Converting categorical columns to numerical codes for prediction...\")\n",
    "    for col_name in non_numerical_cols:\n",
    "        if col_name in X_valid_gpu.columns:\n",
    "            X_valid_gpu[col_name] = X_valid_gpu[col_name].cat.codes\n",
    "\n",
    "    y_valid_gpu = cudf.Series(y_valid.values)\n",
    "    valid_group_df_gpu = cudf.from_pandas(valid_df[GROUP_COLS])\n",
    "\n",
    "    print(\"\\n--- Calculating baseline MAP@7 score on GPU ---\")\n",
    "    baseline_preds_gpu = fil_model.predict(X_valid_gpu)\n",
    "    baseline_score = calculate_map_at_7_gpu(y_valid_gpu, baseline_preds_gpu, valid_group_df_gpu)\n",
    "    print(f\"  -> GPU Baseline MAP@7: {baseline_score:.6f}\")\n",
    "\n",
    "    print(f\"\\n--- Calculating GROUPED permutation importance on GPU for {len(feature_cols)} features ---\")\n",
    "    importances = {}\n",
    "    cp.random.seed(RANDOM_STATE)\n",
    "\n",
    "    shuffle_within_group = lambda x: x.sample(frac=1, random_state=RANDOM_STATE)\n",
    "\n",
    "    for col in tqdm(feature_cols, desc=\"Permuting Features on GPU\"):\n",
    "        original_col_gpu = X_valid_gpu[col].copy()\n",
    "        \n",
    "        permuted_values = original_col_gpu.groupby(\n",
    "            [valid_group_df_gpu['id2'], valid_group_df_gpu['id5']]\n",
    "        ).apply(shuffle_within_group)\n",
    "        \n",
    "        X_valid_gpu[col] = permuted_values.values\n",
    "        \n",
    "        permuted_preds_gpu = fil_model.predict(X_valid_gpu)\n",
    "        permuted_score = calculate_map_at_7_gpu(y_valid_gpu, permuted_preds_gpu, valid_group_df_gpu)\n",
    "        \n",
    "        X_valid_gpu[col] = original_col_gpu\n",
    "        importances[col] = baseline_score - permuted_score\n",
    "\n",
    "    del X_valid_gpu, y_valid_gpu, valid_group_df_gpu, fil_model, tl_model\n",
    "    if cp is not None:\n",
    "        cp.get_default_memory_pool().free_all_blocks()\n",
    "    gc.collect()\n",
    "\n",
    "    return pd.DataFrame.from_dict(importances, orient=\"index\", columns=[\"importance\"])\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the two-stage feature selection process.\n",
    "    \"\"\"\n",
    "    print(\"🚀 Starting Two-Stage GPU-Accelerated Feature Selection\")\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    train_df, valid_df = load_data(TRAIN_INPUT_PATH, VALID_INPUT_PATH)\n",
    "    if train_df is None:\n",
    "        return\n",
    "\n",
    "    base_feature_cols = [col for col in train_df.columns if col not in [TARGET_COL] + GROUP_COLS + ID_COLS_TO_EXCLUDE]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # === CRITICAL CORRECTION: Unify Categorical Feature Definitions ==========\n",
    "    # =========================================================================\n",
    "    # The error occurs because LightGBM requires the categories in categorical\n",
    "    # features to be identical between the training and validation sets.\n",
    "    # We fix this by explicitly defining the categories from the combined data.\n",
    "    print(\"\\n--- Unifying categorical feature definitions across train and valid sets ---\")\n",
    "    for col in base_feature_cols:\n",
    "        if train_df[col].dtype.name in ['category', 'object']:\n",
    "             # Combine categories from both train and valid sets\n",
    "            all_categories = pd.concat([train_df[col], valid_df[col]]).astype('category').cat.categories\n",
    "            # Apply the unified categories to both dataframes\n",
    "            train_df[col] = pd.Categorical(train_df[col], categories=all_categories)\n",
    "            valid_df[col] = pd.Categorical(valid_df[col], categories=all_categories)\n",
    "    # =========================================================================\n",
    "\n",
    "    # --- Prepare full data sets for initial ranking ---\n",
    "    X_train_full = train_df[base_feature_cols]\n",
    "    y_train = train_df[TARGET_COL]\n",
    "    train_groups = train_df.groupby(GROUP_COLS).size().to_numpy()\n",
    "    \n",
    "    X_valid_full = valid_df[base_feature_cols]\n",
    "    y_valid = valid_df[TARGET_COL]\n",
    "    valid_groups = valid_df.groupby(GROUP_COLS).size().to_numpy()\n",
    "\n",
    "    # =========================================================================\n",
    "    # === STAGE 1: Get Candidate Features from a fast LGBM model ================\n",
    "    # =========================================================================\n",
    "    lgbm_full_model = train_lgbm_ranker(\n",
    "        X_train_full, y_train, train_groups, X_valid_full, y_valid, valid_groups, \n",
    "        purpose=\"Initial Candidate Selection\"\n",
    "    )\n",
    "    \n",
    "    importances_df = pd.DataFrame({\n",
    "        'feature': lgbm_full_model.feature_name_,\n",
    "        'importance': lgbm_full_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    candidate_features = importances_df.head(N_CANDIDATE_FEATURES)['feature'].tolist()\n",
    "    print(f\"\\n  -> Selected top {len(candidate_features)} candidate features from Stage 1.\")\n",
    "\n",
    "    del X_train_full, X_valid_full, lgbm_full_model, importances_df\n",
    "    gc.collect()\n",
    "\n",
    "    # =========================================================================\n",
    "    # === STAGE 2: Run Permutation Importance on Candidate Features ===========\n",
    "    # =========================================================================\n",
    "    # Filter data to only include the promising candidates\n",
    "    X_train_cand = train_df[candidate_features]\n",
    "    X_valid_cand = valid_df[candidate_features]\n",
    "\n",
    "    # Train a new model specifically for the permutation importance step\n",
    "    pi_model = train_lgbm_ranker(\n",
    "        X_train_cand, y_train, train_groups, X_valid_cand, y_valid, valid_groups,\n",
    "        purpose=\"Permutation Importance\"\n",
    "    )\n",
    "\n",
    "    # Calculate final importances using the more accurate (but slower) method\n",
    "    final_importance_df = calculate_grouped_permutation_importance(pi_model, X_valid_cand, y_valid, valid_df)\n",
    "    \n",
    "    # --- Process and save results ---\n",
    "    if not final_importance_df.empty:\n",
    "        final_importance_df.sort_values(by=\"importance\", ascending=False, inplace=True)\n",
    "        \n",
    "        full_importance_path = os.path.join(MODEL_DIR, \"full_permutation_importances_gpu.csv\")\n",
    "        final_importance_df.to_csv(full_importance_path)\n",
    "        print(f\"\\n✅ Full permutation importance scores saved to '{full_importance_path}'\")\n",
    "\n",
    "        positive_importance_df = final_importance_df[final_importance_df[\"importance\"] > 0]\n",
    "        top_features = positive_importance_df.head(TOP_N_FEATURES).index.tolist()\n",
    "\n",
    "        print(f\"\\n--- Selected final top {len(top_features)} features from Permutation Importance ---\")\n",
    "        print(\"Top 15 features:\")\n",
    "        print(positive_importance_df.head(15))\n",
    "\n",
    "        selected_features_path = os.path.join(MODEL_DIR, f\"top_{TOP_N_FEATURES}_features_gpu.json\")\n",
    "        with open(selected_features_path, \"w\") as f:\n",
    "            json.dump(top_features, f, indent=4)\n",
    "        print(f\"\\n✅ Top feature list saved to '{selected_features_path}'\")\n",
    "\n",
    "    print(\"\\n--- Feature Selection Script Finished ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if cudf is None:\n",
    "        print(\"❌ This script requires RAPIDS to run. Please install cudf, cuml, and cupy.\")\n",
    "    else:\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5054514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Hyperparameter Tuning for LGBMRanker with Optuna (Revised)\n",
      "\n",
      "--- Step 1: Loading data and top features ---\n",
      "  -> Loaded training data with shape: (589815, 3097)\n",
      "  -> Loaded 50 features from 'model/top_50_features_gpu.json'\n",
      "  -> Loaded data dictionary from 'data_dictionary.csv'\n",
      "\n",
      "--- Step 2: Creating time-based tuning split ---\n",
      "  -> Tuning train shape: (471852, 3097)\n",
      "  -> Tuning valid shape: (117963, 3097)\n",
      "\n",
      "--- Pre-processing data types for LightGBM using Data Dictionary and dtype inspection ---\n",
      "  -> Identified 34 non-numerical features to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-27 19:05:51,251] A new study created in memory with name: lgbm_ranker_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Successfully converted features to 'category' dtype.\n",
      "\n",
      "--- Step 3: Starting Optuna study with 50 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6024769fbdd14997993de50a1739d1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-27 19:06:18,761] Trial 0 finished with value: 0.08036400292848411 and parameters: {'learning_rate': 0.019264162882174963, 'num_leaves': 30, 'max_depth': 10, 'subsample': 0.835030001627035, 'colsample_bytree': 0.6693605035581052, 'reg_alpha': 0.19318856520061312, 'reg_lambda': 0.004839661170136858}. Best is trial 0 with value: 0.08036400292848411.\n",
      "[I 2025-07-27 19:06:24,926] Trial 1 finished with value: 0.08078165985578585 and parameters: {'learning_rate': 0.03380356188343687, 'num_leaves': 44, 'max_depth': 9, 'subsample': 0.9186289871663155, 'colsample_bytree': 0.8183633354819033, 'reg_alpha': 0.14916682279615115, 'reg_lambda': 5.348000217056512}. Best is trial 1 with value: 0.08078165985578585.\n",
      "[I 2025-07-27 19:06:36,815] Trial 2 finished with value: 0.07989251775698254 and parameters: {'learning_rate': 0.010706434693760253, 'num_leaves': 50, 'max_depth': 6, 'subsample': 0.8297891152299803, 'colsample_bytree': 0.8410738827348762, 'reg_alpha': 9.906569505157302, 'reg_lambda': 0.0034624943175259057}. Best is trial 1 with value: 0.08078165985578585.\n",
      "[I 2025-07-27 19:06:42,636] Trial 3 finished with value: 0.08020716515669431 and parameters: {'learning_rate': 0.03177268569768478, 'num_leaves': 26, 'max_depth': 9, 'subsample': 0.7329622587157912, 'colsample_bytree': 0.9303091136851268, 'reg_alpha': 0.12478099571085924, 'reg_lambda': 0.003009325132427436}. Best is trial 1 with value: 0.08078165985578585.\n",
      "[I 2025-07-27 19:06:47,259] Trial 4 finished with value: 0.0799701177711696 and parameters: {'learning_rate': 0.05175751264101655, 'num_leaves': 53, 'max_depth': 8, 'subsample': 0.9262375733054634, 'colsample_bytree': 0.6887390683198302, 'reg_alpha': 0.04362324155510309, 'reg_lambda': 0.08546301995331702}. Best is trial 1 with value: 0.08078165985578585.\n",
      "[I 2025-07-27 19:06:47,942] Trial 5 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:06:48,801] Trial 6 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:06:49,579] Trial 7 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-07-27 19:06:50,380] Trial 8 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:06:51,106] Trial 9 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:06:51,928] Trial 10 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:06:52,846] Trial 11 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-07-27 19:06:53,633] Trial 12 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:06:54,460] Trial 13 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-07-27 19:06:55,296] Trial 14 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:06:56,096] Trial 15 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:06:56,955] Trial 16 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:06:57,765] Trial 17 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:06:58,623] Trial 18 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-07-27 19:07:02,602] Trial 19 finished with value: 0.07995404140883866 and parameters: {'learning_rate': 0.09820838180694887, 'num_leaves': 76, 'max_depth': 10, 'subsample': 0.7021951236688798, 'colsample_bytree': 0.7124631496968654, 'reg_alpha': 0.220013902702066, 'reg_lambda': 0.011492105915500322}. Best is trial 1 with value: 0.08078165985578585.\n",
      "[I 2025-07-27 19:07:03,399] Trial 20 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:07:04,212] Trial 21 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:07:04,970] Trial 22 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:05,766] Trial 23 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:07:06,611] Trial 24 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:07,470] Trial 25 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:08,201] Trial 26 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:07:09,133] Trial 27 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:09,892] Trial 28 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:10,713] Trial 29 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:07:11,541] Trial 30 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:12,377] Trial 31 pruned. Trial was pruned at iteration 3.\n",
      "[I 2025-07-27 19:07:17,615] Trial 32 finished with value: 0.08034863773599425 and parameters: {'learning_rate': 0.05053547561600421, 'num_leaves': 59, 'max_depth': 8, 'subsample': 0.8697858737721674, 'colsample_bytree': 0.7003526191881131, 'reg_alpha': 0.0327006249587636, 'reg_lambda': 0.492112919025558}. Best is trial 1 with value: 0.08078165985578585.\n",
      "[I 2025-07-27 19:07:18,413] Trial 33 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:19,237] Trial 34 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-07-27 19:07:20,190] Trial 35 pruned. Trial was pruned at iteration 1.\n",
      "[I 2025-07-27 19:07:21,009] Trial 36 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:21,794] Trial 37 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:07:22,597] Trial 38 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:07:23,387] Trial 39 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:07:24,246] Trial 40 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:28,732] Trial 41 finished with value: 0.08080645543675898 and parameters: {'learning_rate': 0.07695333505990584, 'num_leaves': 56, 'max_depth': 8, 'subsample': 0.9320390657202683, 'colsample_bytree': 0.6654652754818798, 'reg_alpha': 0.24351049697325652, 'reg_lambda': 0.08347593797727203}. Best is trial 41 with value: 0.08080645543675898.\n",
      "[I 2025-07-27 19:07:29,533] Trial 42 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:33,623] Trial 43 finished with value: 0.08064770465584371 and parameters: {'learning_rate': 0.09543721728470696, 'num_leaves': 67, 'max_depth': 7, 'subsample': 0.9160431930761495, 'colsample_bytree': 0.7283175911910302, 'reg_alpha': 0.22832493405768892, 'reg_lambda': 0.021046719324101692}. Best is trial 41 with value: 0.08080645543675898.\n",
      "[I 2025-07-27 19:07:34,413] Trial 44 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:35,209] Trial 45 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:07:35,946] Trial 46 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:07:36,780] Trial 47 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-07-27 19:07:37,619] Trial 48 pruned. Trial was pruned at iteration 2.\n",
      "[I 2025-07-27 19:07:38,557] Trial 49 pruned. Trial was pruned at iteration 0.\n",
      "\n",
      "--- Study Complete ---\n",
      "  -> Best MAP@7 score: 0.080806\n",
      "  -> Best hyperparameters found:\n",
      "       - learning_rate: 0.07695333505990584\n",
      "       - num_leaves: 56\n",
      "       - max_depth: 8\n",
      "       - subsample: 0.9320390657202683\n",
      "       - colsample_bytree: 0.6654652754818798\n",
      "       - reg_alpha: 0.24351049697325652\n",
      "       - reg_lambda: 0.08347593797727203\n",
      "\n",
      "✅ Best parameters saved to 'model/best_lgbm_params.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR = \"inter\"\n",
    "TRAIN_INPUT_PATH = f\"{INPUT_DIR}/train_3_model_ready.parquet\"\n",
    "MODEL_DIR = \"model\"\n",
    "TOP_FEATURES_PATH = f\"{MODEL_DIR}/top_50_features_gpu.json\"\n",
    "DATA_DICTIONARY_PATH = \"data_dictionary.csv\" # Path to your data dictionary\n",
    "\n",
    "# --- Optuna Settings ---\n",
    "N_TRIALS = 50\n",
    "TUNING_VALIDATION_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def calculate_map_at_7(y_true, y_pred, groups_df):\n",
    "    \"\"\"\n",
    "    Calculates Mean Average Precision @ 7, based on the competition's logic.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"y\": y_true.values, \"pred\": y_pred})\n",
    "    df = pd.concat([groups_df.reset_index(drop=True), df], axis=1)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # === CRITICAL CORRECTION: Fix sorting for ranking metric calculation =====\n",
    "    # =========================================================================\n",
    "    # The groups (id2, id5) must be sorted ascending to keep them together,\n",
    "    # while the predictions (pred) must be sorted descending to rank them.\n",
    "    df = df.sort_values([\"id2\", \"id5\", \"pred\"], ascending=[True, True, False]).reset_index(drop=True)\n",
    "    # =========================================================================\n",
    "\n",
    "    df[\"cumsum_y\"] = df.groupby([\"id2\", \"id5\"])[\"y\"].cumsum()\n",
    "    df[\"position\"] = df.groupby([\"id2\", \"id5\"]).cumcount() + 1\n",
    "    df[\"ap_numerator_term\"] = (df[\"cumsum_y\"] / df[\"position\"]) * df[\"y\"]\n",
    "\n",
    "    ap_df = df.groupby([\"id2\", \"id5\"]).agg(\n",
    "        map_numerator=(\"ap_numerator_term\", \"sum\"),\n",
    "        map_denominator=(\"y\", lambda x: min(x.sum(), 7)),\n",
    "    )\n",
    "    ap_df[\"ap_at_7\"] = ap_df.apply(\n",
    "        lambda row: row[\"map_numerator\"] / row[\"map_denominator\"]\n",
    "        if row[\"map_denominator\"] > 0\n",
    "        else 0,\n",
    "        axis=1,\n",
    "    )\n",
    "    return ap_df[\"ap_at_7\"].mean()\n",
    "\n",
    "\n",
    "def objective(\n",
    "    trial, tune_train_df, tune_valid_df, feature_cols\n",
    "):\n",
    "    \"\"\"\n",
    "    The objective function that Optuna will try to maximize.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"objective\": \"lambdarank\",\n",
    "        \"metric\": \"map\",\n",
    "        \"eval_at\": [7],\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"n_jobs\": -1,\n",
    "        \"verbose\": -1,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 80),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "    X_train, y_train = tune_train_df[feature_cols], tune_train_df[\"y\"]\n",
    "    X_valid, y_valid = tune_valid_df[feature_cols], tune_valid_df[\"y\"]\n",
    "\n",
    "    train_groups = tune_train_df.groupby([\"id2\", \"id5\"]).size().to_numpy()\n",
    "    valid_groups = tune_valid_df.groupby([\"id2\", \"id5\"]).size().to_numpy()\n",
    "\n",
    "    pruning_callback = LightGBMPruningCallback(trial, \"map@7\")\n",
    "\n",
    "    model = lgb.LGBMRanker(**params, n_estimators=1000)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        group=train_groups,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_group=[valid_groups],\n",
    "        callbacks=[lgb.early_stopping(50, verbose=False), pruning_callback],\n",
    "        categorical_feature='auto', # Let LightGBM handle columns with 'category' dtype\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_valid)\n",
    "    valid_groups_df = tune_valid_df[[\"id2\", \"id5\"]]\n",
    "    score = calculate_map_at_7(y_valid, preds, valid_groups_df)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting Hyperparameter Tuning for LGBMRanker with Optuna (Revised)\")\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"\\n--- Step 1: Loading data and top features ---\")\n",
    "    try:\n",
    "        train_df = pd.read_parquet(TRAIN_INPUT_PATH)\n",
    "        with open(TOP_FEATURES_PATH, \"r\") as f:\n",
    "            top_features = json.load(f)\n",
    "        # Load the data dictionary to define data types\n",
    "        data_dict = pd.read_csv(DATA_DICTIONARY_PATH)\n",
    "        print(f\"  -> Loaded training data with shape: {train_df.shape}\")\n",
    "        print(f\"  -> Loaded {len(top_features)} features from '{TOP_FEATURES_PATH}'\")\n",
    "        print(f\"  -> Loaded data dictionary from '{DATA_DICTIONARY_PATH}'\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            f\"❌ ERROR: Could not find a required file. Please run previous steps. Details: {e}\"\n",
    "        )\n",
    "        exit()\n",
    "\n",
    "    print(\"\\n--- Step 2: Creating time-based tuning split ---\")\n",
    "    # The data should already be sorted by time from previous steps, but we ensure it here.\n",
    "    train_df[\"id5\"] = pd.to_datetime(train_df[\"id5\"])\n",
    "    train_df = train_df.sort_values(\"id5\").reset_index(drop=True)\n",
    "\n",
    "    split_point = int(len(train_df) * (1 - TUNING_VALIDATION_RATIO))\n",
    "    tune_train_df = train_df.iloc[:split_point]\n",
    "    tune_valid_df = train_df.iloc[split_point:]\n",
    "\n",
    "    print(f\"  -> Tuning train shape: {tune_train_df.shape}\")\n",
    "    print(f\"  -> Tuning valid shape: {tune_valid_df.shape}\")\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    # =========================================================================\n",
    "    # === CRITICAL CORRECTION: Pre-process dtypes using a Robust Strategy ====\n",
    "    # =========================================================================\n",
    "    # Use the data dictionary and dtype inspection to build a comprehensive list\n",
    "    # of all non-numerical features, ensuring none are missed.\n",
    "    print(\"\\n--- Pre-processing data types for LightGBM using Data Dictionary and dtype inspection ---\")\n",
    "    \n",
    "    # 1. Get categorical features from the data dictionary\n",
    "    dict_cat_cols = data_dict[data_dict['Type'] == 'Categorical']['masked_column'].tolist()\n",
    "    \n",
    "    # 2. Get any other non-numerical features directly from the DataFrame dtypes\n",
    "    # This catches any feature-engineered columns that might not be in the dictionary.\n",
    "    other_obj_cols = tune_train_df[top_features].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # 3. Combine them into a single, unique list\n",
    "    categorical_features_to_convert = sorted(list(set(dict_cat_cols + other_obj_cols)))\n",
    "    \n",
    "    # 4. Filter this list to only include features present in our `top_features` list\n",
    "    final_cat_cols = [col for col in categorical_features_to_convert if col in top_features]\n",
    "\n",
    "    print(f\"  -> Identified {len(final_cat_cols)} non-numerical features to process.\")\n",
    "\n",
    "    for col in final_cat_cols:\n",
    "        # Unify categories between train and valid sets to prevent errors\n",
    "        all_categories = pd.concat([tune_train_df[col], tune_valid_df[col]]).astype('category').cat.categories\n",
    "        tune_train_df[col] = pd.Categorical(tune_train_df[col], categories=all_categories)\n",
    "        tune_valid_df[col] = pd.Categorical(tune_valid_df[col], categories=all_categories)\n",
    "    \n",
    "    print(f\"  -> Successfully converted features to 'category' dtype.\")\n",
    "    # =========================================================================\n",
    "\n",
    "    print(f\"\\n--- Step 3: Starting Optuna study with {N_TRIALS} trials ---\")\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=\"lgbm_ranker_tuning\")\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(\n",
    "            trial,\n",
    "            tune_train_df,\n",
    "            tune_valid_df,\n",
    "            top_features,\n",
    "        ),\n",
    "        n_trials=N_TRIALS,\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"\\n--- Study Complete ---\")\n",
    "    print(f\"  -> Best MAP@7 score: {study.best_value:.6f}\")\n",
    "    print(\"  -> Best hyperparameters found:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"       - {key}: {value}\")\n",
    "\n",
    "    best_params.update(\n",
    "        {\n",
    "            \"objective\": \"lambdarank\",\n",
    "            \"metric\": \"map\",\n",
    "            \"eval_at\": [7],\n",
    "            \"boosting_type\": \"gbdt\",\n",
    "            \"seed\": RANDOM_STATE,\n",
    "            \"n_jobs\": -1,\n",
    "            \"verbose\": -1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    params_path = os.path.join(MODEL_DIR, \"best_lgbm_params.json\")\n",
    "    with open(params_path, \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    print(f\"\\n✅ Best parameters saved to '{params_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2503d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting LGBM Out-of-Fold (OOF) Prediction Generation (Revised)\n",
      "\n",
      "--- Step 1: Loading data and artifacts ---\n",
      "  -> Loaded training data with shape: (589815, 3097)\n",
      "  -> Loaded 50 features from 'model/top_50_features_gpu.json'\n",
      "  -> Loaded best hyperparameters from 'model/best_lgbm_params.json'\n",
      "  -> Loaded data dictionary from 'data_dictionary.csv'\n",
      "\n",
      "--- Pre-processing data types for LightGBM using Data Dictionary and dtype inspection ---\n",
      "  -> Identified 34 non-numerical features to process.\n",
      "  -> Successfully converted features to 'category' dtype across all data splits.\n",
      "\n",
      "--- Step 2: Preparing data with selected features ---\n",
      "\n",
      "--- Step 3: Generating OOF predictions with LGBMRanker ---\n",
      "\n",
      "  -> Processing Fold 1/5...\n",
      "  -> Model for fold 1 saved to 'model/lgbm_ranker_fold_1.txt'\n",
      "\n",
      "  -> Processing Fold 2/5...\n",
      "  -> Model for fold 2 saved to 'model/lgbm_ranker_fold_2.txt'\n",
      "\n",
      "  -> Processing Fold 3/5...\n",
      "  -> Model for fold 3 saved to 'model/lgbm_ranker_fold_3.txt'\n",
      "\n",
      "  -> Processing Fold 4/5...\n",
      "  -> Model for fold 4 saved to 'model/lgbm_ranker_fold_4.txt'\n",
      "\n",
      "  -> Processing Fold 5/5...\n",
      "  -> Model for fold 5 saved to 'model/lgbm_ranker_fold_5.txt'\n",
      "\n",
      "  -> OOF and Ensemble predictions created successfully.\n",
      "\n",
      "--- Step 4: Saving new datasets with OOF features ---\n",
      "  -> Saved 'train_4_oof_lgbm.parquet' with shape (589815, 3098)\n",
      "  -> Saved 'valid_4_oof_lgbm.parquet' with shape (180349, 3099)\n",
      "  -> Saved 'test_4_oof_lgbm.parquet' with shape (337714, 3099)\n",
      "\n",
      "✅ --- LGBM OOF Feature Generation Complete --- ✅\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR = \"inter\"\n",
    "TRAIN_INPUT_PATH = f\"{INPUT_DIR}/train_3_model_ready.parquet\"\n",
    "VALID_INPUT_PATH = f\"{INPUT_DIR}/valid_3_model_ready.parquet\"\n",
    "TEST_INPUT_PATH = f\"{INPUT_DIR}/test_3_model_ready.parquet\"\n",
    "DATA_DICTIONARY_PATH = \"data_dictionary.csv\" # Path to your data dictionary\n",
    "\n",
    "MODEL_DIR = \"model\"\n",
    "OUTPUT_DIR = \"inter\"\n",
    "\n",
    "# Paths to the artifacts created by previous scripts\n",
    "TOP_FEATURES_PATH = f\"{MODEL_DIR}/top_50_features_gpu.json\"\n",
    "BEST_PARAMS_PATH = f\"{MODEL_DIR}/best_lgbm_params.json\"\n",
    "\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ==============================================================================\n",
    "# === MAIN EXECUTION ===========================================================\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting LGBM Out-of-Fold (OOF) Prediction Generation (Revised)\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    # --- Step 1: Load Data and Modeling Artifacts ---\n",
    "    print(\"\\n--- Step 1: Loading data and artifacts ---\")\n",
    "    try:\n",
    "        train_df = pd.read_parquet(TRAIN_INPUT_PATH)\n",
    "        valid_df = pd.read_parquet(VALID_INPUT_PATH)\n",
    "        test_df = pd.read_parquet(TEST_INPUT_PATH)\n",
    "\n",
    "        with open(TOP_FEATURES_PATH, \"r\") as f:\n",
    "            top_features = json.load(f)\n",
    "\n",
    "        with open(BEST_PARAMS_PATH, \"r\") as f:\n",
    "            lgbm_params = json.load(f)\n",
    "            \n",
    "        data_dict = pd.read_csv(DATA_DICTIONARY_PATH)\n",
    "\n",
    "        print(f\"  -> Loaded training data with shape: {train_df.shape}\")\n",
    "        print(f\"  -> Loaded {len(top_features)} features from '{TOP_FEATURES_PATH}'\")\n",
    "        print(f\"  -> Loaded best hyperparameters from '{BEST_PARAMS_PATH}'\")\n",
    "        print(f\"  -> Loaded data dictionary from '{DATA_DICTIONARY_PATH}'\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ ERROR: Could not find a required file. Please run all previous steps. Details: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # =========================================================================\n",
    "    # === CRITICAL CORRECTION: Pre-process dtypes using a Robust Strategy ====\n",
    "    # =========================================================================\n",
    "    # Use the data dictionary and dtype inspection to build a comprehensive list\n",
    "    # of all non-numerical features, ensuring none are missed.\n",
    "    print(\"\\n--- Pre-processing data types for LightGBM using Data Dictionary and dtype inspection ---\")\n",
    "    \n",
    "    # 1. Get categorical features from the data dictionary\n",
    "    dict_cat_cols = data_dict[data_dict['Type'] == 'Categorical']['masked_column'].tolist()\n",
    "    \n",
    "    # 2. Get any other non-numerical features directly from the DataFrame dtypes\n",
    "    other_obj_cols = train_df[top_features].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # 3. Combine them into a single, unique list\n",
    "    categorical_features_to_convert = sorted(list(set(dict_cat_cols + other_obj_cols)))\n",
    "    \n",
    "    # 4. Filter this list to only include features present in our `top_features` list\n",
    "    final_cat_cols = [col for col in categorical_features_to_convert if col in top_features]\n",
    "\n",
    "    print(f\"  -> Identified {len(final_cat_cols)} non-numerical features to process.\")\n",
    "\n",
    "    for col in final_cat_cols:\n",
    "        # Unify categories across all data splits to prevent errors\n",
    "        all_categories = pd.concat([train_df[col], valid_df[col], test_df[col]]).astype('category').cat.categories\n",
    "        train_df[col] = pd.Categorical(train_df[col], categories=all_categories)\n",
    "        valid_df[col] = pd.Categorical(valid_df[col], categories=all_categories)\n",
    "        test_df[col] = pd.Categorical(test_df[col], categories=all_categories)\n",
    "    \n",
    "    print(f\"  -> Successfully converted features to 'category' dtype across all data splits.\")\n",
    "    # =========================================================================\n",
    "\n",
    "    # --- Step 2: Prepare DataFrames for Ranking ---\n",
    "    print(\"\\n--- Step 2: Preparing data with selected features ---\")\n",
    "    # Sort all dataframes by customer and time to correctly calculate group sizes later\n",
    "    train_df = train_df.sort_values(by=[\"id2\", \"id5\", \"id4\"]).reset_index(drop=True)\n",
    "    valid_df = valid_df.sort_values(by=[\"id2\", \"id5\", \"id4\"]).reset_index(drop=True)\n",
    "    test_df = test_df.sort_values(by=[\"id2\", \"id5\", \"id4\"]).reset_index(drop=True)\n",
    "\n",
    "    target_col = \"y\"\n",
    "    X_train = train_df[top_features]\n",
    "    y_train = train_df[target_col]\n",
    "    groups_for_split = train_df[\"id2\"]\n",
    "\n",
    "    X_valid = valid_df[top_features]\n",
    "    X_test = test_df[top_features]\n",
    "\n",
    "    # --- Step 3: OOF Generation with StratifiedGroupKFold ---\n",
    "    print(\"\\n--- Step 3: Generating OOF predictions with LGBMRanker ---\")\n",
    "    sgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    oof_preds = np.zeros(len(train_df))\n",
    "    valid_preds_ensemble = np.zeros(len(valid_df))\n",
    "    test_preds_ensemble = np.zeros(len(test_df))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        sgkf.split(X_train, y_train, groups=groups_for_split)\n",
    "    ):\n",
    "        print(f\"\\n  -> Processing Fold {fold + 1}/{N_SPLITS}...\")\n",
    "        X_train_fold, y_train_fold = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "        X_val_fold, y_val_fold = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        train_fold_groups = train_df.iloc[train_idx].groupby([\"id2\", \"id5\"]).size().to_numpy()\n",
    "        val_fold_groups = train_df.iloc[val_idx].groupby([\"id2\", \"id5\"]).size().to_numpy()\n",
    "\n",
    "        model = lgb.LGBMRanker(**lgbm_params, n_estimators=2000)\n",
    "        model.fit(\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            group=train_fold_groups,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            eval_group=[val_fold_groups],\n",
    "            callbacks=[lgb.early_stopping(100, verbose=False)],\n",
    "            categorical_feature='auto', # Let LightGBM handle categoricals automatically\n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_DIR, f\"lgbm_ranker_fold_{fold + 1}.txt\")\n",
    "        model.booster_.save_model(model_path)\n",
    "        print(f\"  -> Model for fold {fold + 1} saved to '{model_path}'\")\n",
    "\n",
    "        oof_preds[val_idx] = model.predict(X_val_fold)\n",
    "        valid_preds_ensemble += model.predict(X_valid) / N_SPLITS\n",
    "        test_preds_ensemble += model.predict(X_test) / N_SPLITS\n",
    "\n",
    "    train_df[\"oof_lgbm_prediction\"] = oof_preds\n",
    "    valid_df[\"oof_lgbm_prediction\"] = valid_preds_ensemble\n",
    "    test_df[\"oof_lgbm_prediction\"] = test_preds_ensemble\n",
    "    print(\"\\n  -> OOF and Ensemble predictions created successfully.\")\n",
    "\n",
    "    # --- Step 4: Save the final datasets ---\n",
    "    print(\"\\n--- Step 4: Saving new datasets with OOF features ---\")\n",
    "    train_df.to_parquet(f\"{OUTPUT_DIR}/train_4_oof_lgbm.parquet\", index=False)\n",
    "    valid_df.to_parquet(f\"{OUTPUT_DIR}/valid_4_oof_lgbm.parquet\", index=False)\n",
    "    test_df.to_parquet(f\"{OUTPUT_DIR}/test_4_oof_lgbm.parquet\", index=False)\n",
    "\n",
    "    print(f\"  -> Saved 'train_4_oof_lgbm.parquet' with shape {train_df.shape}\")\n",
    "    print(f\"  -> Saved 'valid_4_oof_lgbm.parquet' with shape {valid_df.shape}\")\n",
    "    print(f\"  -> Saved 'test_4_oof_lgbm.parquet' with shape {test_df.shape}\")\n",
    "\n",
    "    del train_df, valid_df, test_df, X_train, y_train, X_valid, X_test\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"\\n✅ --- LGBM OOF Feature Generation Complete --- ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e9047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Hyperparameter Tuning for XGBRanker with Optuna\n",
      "\n",
      "--- Step 1: Loading data and top features ---\n",
      "  -> Loaded training data with shape: (589815, 3097)\n",
      "  -> Loaded 50 features from 'model/top_50_features_gpu.json'\n",
      "  -> Loaded data dictionary from 'data_dictionary.csv'\n",
      "\n",
      "--- Step 2: Creating time-based tuning split ---\n",
      "  -> Tuning train shape: (471852, 3097)\n",
      "  -> Tuning valid shape: (117963, 3097)\n",
      "\n",
      "--- Step 3: Preparing data for XGBoost ---\n",
      "  -> Identifying non-numerical features to convert...\n",
      "  -> Found 34 features to convert to 'category' dtype.\n",
      "  -> Successfully converted features and unified categories.\n",
      "  -> Sorting dataframes by query ID to meet XGBoost requirements...\n",
      "  -> Creating Query IDs (qid) for XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-27 19:04:19,819] A new study created in memory with name: xgb_ranker_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 4: Starting Optuna study with 20 trials ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48d9d4d75fb4efbbbd325659e64a5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "\n",
    "# --- Suppress Warnings for a Cleaner Output ---\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR = \"inter\"\n",
    "TRAIN_INPUT_PATH = f\"{INPUT_DIR}/train_3_model_ready.parquet\"\n",
    "MODEL_DIR = \"model\"\n",
    "TOP_FEATURES_PATH = f\"{MODEL_DIR}/top_50_features_gpu.json\"\n",
    "DATA_DICTIONARY_PATH = \"data_dictionary.csv\"\n",
    "\n",
    "# --- Optuna Settings ---\n",
    "N_TRIALS = 20\n",
    "TUNING_VALIDATION_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "def calculate_map_at_7(y_true, y_pred, groups_df):\n",
    "    \"\"\"\n",
    "    Calculates Mean Average Precision @ 7.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"y\": y_true, \"pred\": y_pred})\n",
    "    df = pd.concat([groups_df.reset_index(drop=True), df], axis=1)\n",
    "    df = df.sort_values([\"id2\", \"id5\", \"pred\"], ascending=[True, True, False]).reset_index(drop=True)\n",
    "    df[\"cumsum_y\"] = df.groupby([\"id2\", \"id5\"])[\"y\"].cumsum()\n",
    "    df[\"position\"] = df.groupby([\"id2\", \"id5\"]).cumcount() + 1\n",
    "    df[\"ap_numerator_term\"] = (df[\"cumsum_y\"] / df[\"position\"]) * df[\"y\"]\n",
    "    ap_df = df.groupby([\"id2\", \"id5\"]).agg(\n",
    "        map_numerator=(\"ap_numerator_term\", \"sum\"),\n",
    "        map_denominator=(\"y\", lambda x: min(x.sum(), 7)),\n",
    "    )\n",
    "    ap_df[\"ap_at_7\"] = ap_df.apply(\n",
    "        lambda row: row[\"map_numerator\"] / row[\"map_denominator\"]\n",
    "        if row[\"map_denominator\"] > 0\n",
    "        else 0,\n",
    "        axis=1,\n",
    "    )\n",
    "    return ap_df[\"ap_at_7\"].mean()\n",
    "\n",
    "\n",
    "def objective(trial, tune_train_df, tune_valid_df, feature_cols, qid_train, qid_valid):\n",
    "    \"\"\"\n",
    "    The objective function for XGBoost that Optuna will try to maximize.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"objective\": \"rank:map\",\n",
    "        \"eval_metric\": \"map@7\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"seed\": RANDOM_STATE,\n",
    "        \"n_jobs\": -1,\n",
    "        \"enable_categorical\": True,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"eta\": trial.suggest_float(\"eta\", 0.01, 0.1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),\n",
    "    }\n",
    "\n",
    "    X_train, y_train = tune_train_df[feature_cols], tune_train_df[\"y\"]\n",
    "    X_valid, y_valid = tune_valid_df[feature_cols], tune_valid_df[\"y\"]\n",
    "\n",
    "    pruning_callback = XGBoostPruningCallback(trial, \"validation_0-map@7\")\n",
    "\n",
    "    model = xgb.XGBRanker(\n",
    "        **params, \n",
    "        n_estimators=1000,\n",
    "        callbacks=[pruning_callback]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        qid=qid_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_qid=[qid_valid],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    preds = model.predict(X_valid)\n",
    "    valid_groups_df = tune_valid_df[[\"id2\", \"id5\"]]\n",
    "    score = calculate_map_at_7(y_valid, preds, valid_groups_df)\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting Hyperparameter Tuning for XGBRanker with Optuna\")\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"\\n--- Step 1: Loading data and top features ---\")\n",
    "    try:\n",
    "        train_df = pd.read_parquet(TRAIN_INPUT_PATH)\n",
    "        with open(TOP_FEATURES_PATH, \"r\") as f:\n",
    "            top_features = json.load(f)\n",
    "        data_dict = pd.read_csv(DATA_DICTIONARY_PATH)\n",
    "        print(f\"  -> Loaded training data with shape: {train_df.shape}\")\n",
    "        print(f\"  -> Loaded {len(top_features)} features from '{TOP_FEATURES_PATH}'\")\n",
    "        print(f\"  -> Loaded data dictionary from '{DATA_DICTIONARY_PATH}'\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ ERROR: Could not find a required file. Please run previous steps. Details: {e}\")\n",
    "        exit()\n",
    "\n",
    "    print(\"\\n--- Step 2: Creating time-based tuning split ---\")\n",
    "    train_df[\"id5\"] = pd.to_datetime(train_df[\"id5\"])\n",
    "    train_df = train_df.sort_values(\"id5\").reset_index(drop=True)\n",
    "    split_point = int(len(train_df) * (1 - TUNING_VALIDATION_RATIO))\n",
    "    tune_train_df = train_df.iloc[:split_point].copy()\n",
    "    tune_valid_df = train_df.iloc[split_point:].copy()\n",
    "    print(f\"  -> Tuning train shape: {tune_train_df.shape}\")\n",
    "    print(f\"  -> Tuning valid shape: {tune_valid_df.shape}\")\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"\\n--- Step 3: Preparing data for XGBoost ---\")\n",
    "    print(\"  -> Identifying non-numerical features to convert...\")\n",
    "    dict_cat_cols = data_dict[data_dict['Type'] == 'Categorical']['masked_column'].tolist()\n",
    "    other_obj_cols = tune_train_df[top_features].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    categorical_features_to_convert = sorted(list(set(dict_cat_cols + other_obj_cols)))\n",
    "    final_cat_cols = [col for col in categorical_features_to_convert if col in top_features]\n",
    "    print(f\"  -> Found {len(final_cat_cols)} features to convert to 'category' dtype.\")\n",
    "\n",
    "    for col in final_cat_cols:\n",
    "        all_categories = pd.concat([tune_train_df[col], tune_valid_df[col]]).astype('category').cat.categories\n",
    "        tune_train_df[col] = pd.Categorical(tune_train_df[col], categories=all_categories)\n",
    "        tune_valid_df[col] = pd.Categorical(tune_valid_df[col], categories=all_categories)\n",
    "    print(\"  -> Successfully converted features and unified categories.\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # === ✨ CRITICAL FIX: Sort dataframes to group queries together ✨ ===\n",
    "    # =========================================================================\n",
    "    print(\"  -> Sorting dataframes by query ID to meet XGBoost requirements...\")\n",
    "    tune_train_df = tune_train_df.sort_values([\"id2\", \"id5\"]).reset_index(drop=True)\n",
    "    tune_valid_df = tune_valid_df.sort_values([\"id2\", \"id5\"]).reset_index(drop=True)\n",
    "    # =========================================================================\n",
    "\n",
    "    print(\"  -> Creating Query IDs (qid) for XGBoost...\")\n",
    "    qid_train = pd.factorize(tune_train_df[\"id2\"].astype(str) + \"_\" + tune_train_df[\"id5\"].astype(str))[0]\n",
    "    qid_valid = pd.factorize(tune_valid_df[\"id2\"].astype(str) + \"_\" + tune_valid_df[\"id5\"].astype(str))[0]\n",
    "\n",
    "    print(f\"\\n--- Step 4: Starting Optuna study with {N_TRIALS} trials ---\")\n",
    "    study = optuna.create_study(direction=\"maximize\", study_name=\"xgb_ranker_tuning\")\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(\n",
    "            trial, tune_train_df, tune_valid_df, top_features, qid_train, qid_valid\n",
    "        ),\n",
    "        n_trials=N_TRIALS,\n",
    "        show_progress_bar=True,\n",
    "    )\n",
    "\n",
    "    best_params = study.best_params\n",
    "    print(\"\\n--- Study Complete ---\")\n",
    "    print(f\"  -> Best MAP@7 score: {study.best_value:.6f}\")\n",
    "    print(\"  -> Best hyperparameters found:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"        - {key}: {value}\")\n",
    "\n",
    "    best_params.update(\n",
    "        {\n",
    "            \"objective\": \"rank:map\",\n",
    "            \"eval_metric\": \"map@7\",\n",
    "            \"booster\": \"gbtree\",\n",
    "            \"seed\": RANDOM_STATE,\n",
    "            \"n_jobs\": -1,\n",
    "            \"enable_categorical\": True,\n",
    "            \"tree_method\": \"hist\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    params_path = os.path.join(MODEL_DIR, \"best_xgb_params.json\")\n",
    "    with open(params_path, \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    print(f\"\\n✅ Best XGBoost parameters saved to '{params_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_DIR = \"inter\"\n",
    "# NOTE: Using the output from the LGBM OOF script as input\n",
    "TRAIN_INPUT_PATH = f\"{INPUT_DIR}/train_4_oof_lgbm.parquet\"\n",
    "VALID_INPUT_PATH = f\"{INPUT_DIR}/valid_4_oof_lgbm.parquet\"\n",
    "TEST_INPUT_PATH = f\"{INPUT_DIR}/test_4_oof_lgbm.parquet\"\n",
    "DATA_DICTIONARY_PATH = \"data_dictionary.csv\" # Path to your data dictionary\n",
    "\n",
    "MODEL_DIR = \"model\"\n",
    "OUTPUT_DIR = \"inter\"\n",
    "\n",
    "# Paths to the artifacts created by previous scripts\n",
    "TOP_FEATURES_PATH = f\"{MODEL_DIR}/top_50_features_gpu.json\"\n",
    "BEST_PARAMS_PATH = f\"{MODEL_DIR}/best_xgb_params.json\"\n",
    "\n",
    "N_SPLITS = 5\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ==============================================================================\n",
    "# === MAIN EXECUTION ===========================================================\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting XGBoost Out-of-Fold (OOF) Prediction Generation\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "    # --- Step 1: Load Data and Modeling Artifacts ---\n",
    "    print(\"\\n--- Step 1: Loading data and artifacts ---\")\n",
    "    try:\n",
    "        train_df = pd.read_parquet(TRAIN_INPUT_PATH)\n",
    "        valid_df = pd.read_parquet(VALID_INPUT_PATH)\n",
    "        test_df = pd.read_parquet(TEST_INPUT_PATH)\n",
    "\n",
    "        with open(TOP_FEATURES_PATH, \"r\") as f:\n",
    "            top_features = json.load(f)\n",
    "\n",
    "        with open(BEST_PARAMS_PATH, \"r\") as f:\n",
    "            xgb_params = json.load(f)\n",
    "            \n",
    "        data_dict = pd.read_csv(DATA_DICTIONARY_PATH)\n",
    "\n",
    "        print(f\"  -> Loaded training data with shape: {train_df.shape}\")\n",
    "        print(f\"  -> Loaded {len(top_features)} features from '{TOP_FEATURES_PATH}'\")\n",
    "        print(f\"  -> Loaded best XGBoost hyperparameters from '{BEST_PARAMS_PATH}'\")\n",
    "        print(f\"  -> Loaded data dictionary from '{DATA_DICTIONARY_PATH}'\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            f\"❌ ERROR: Could not find a required file. Please run all previous steps. Details: {e}\"\n",
    "        )\n",
    "        exit()\n",
    "\n",
    "    # =========================================================================\n",
    "    # === CRITICAL CORRECTION: Pre-process dtypes using a Robust Strategy ====\n",
    "    # =========================================================================\n",
    "    # Use the data dictionary and dtype inspection to build a comprehensive list\n",
    "    # of all non-numerical features, ensuring none are missed.\n",
    "    print(\"\\n--- Pre-processing data types for XGBoost using Data Dictionary and dtype inspection ---\")\n",
    "    \n",
    "    # 1. Get categorical features from the data dictionary\n",
    "    dict_cat_cols = data_dict[data_dict['Type'] == 'Categorical']['masked_column'].tolist()\n",
    "    \n",
    "    # 2. Get any other non-numerical features directly from the DataFrame dtypes\n",
    "    other_obj_cols = train_df[top_features].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # 3. Combine them into a single, unique list\n",
    "    categorical_features_to_convert = sorted(list(set(dict_cat_cols + other_obj_cols)))\n",
    "    \n",
    "    # 4. Filter this list to only include features present in our `top_features` list\n",
    "    final_cat_cols = [col for col in categorical_features_to_convert if col in top_features]\n",
    "\n",
    "    print(f\"  -> Identified {len(final_cat_cols)} non-numerical features to process.\")\n",
    "\n",
    "    for col in final_cat_cols:\n",
    "        # Unify categories across all data splits to prevent errors\n",
    "        all_categories = pd.concat([train_df[col], valid_df[col], test_df[col]]).astype('category').cat.categories\n",
    "        train_df[col] = pd.Categorical(train_df[col], categories=all_categories)\n",
    "        valid_df[col] = pd.Categorical(valid_df[col], categories=all_categories)\n",
    "        test_df[col] = pd.Categorical(test_df[col], categories=all_categories)\n",
    "    \n",
    "    print(f\"  -> Successfully converted features to 'category' dtype across all data splits.\")\n",
    "    # =========================================================================\n",
    "\n",
    "    # --- Step 2: Prepare DataFrames for XGBoost Ranking ---\n",
    "    print(\"\\n--- Step 2: Preparing data with selected features ---\")\n",
    "    train_df = train_df.sort_values(by=[\"id2\", \"id5\", \"id4\"]).reset_index(drop=True)\n",
    "    valid_df = valid_df.sort_values(by=[\"id2\", \"id5\", \"id4\"]).reset_index(drop=True)\n",
    "    test_df = test_df.sort_values(by=[\"id2\", \"id5\", \"id4\"]).reset_index(drop=True)\n",
    "\n",
    "    # Add the LGBM OOF prediction as a feature\n",
    "    all_features = top_features + [\"oof_lgbm_prediction\"]\n",
    "\n",
    "    target_col = \"y\"\n",
    "    X_train = train_df[all_features]\n",
    "    y_train = train_df[target_col]\n",
    "    groups_for_split = train_df[\"id2\"]\n",
    "\n",
    "    X_valid = valid_df[all_features]\n",
    "    X_test = test_df[all_features]\n",
    "\n",
    "    # --- Step 3: OOF Generation with StratifiedGroupKFold ---\n",
    "    print(\"\\n--- Step 3: Generating OOF predictions with XGBRanker ---\")\n",
    "    sgkf = StratifiedGroupKFold(\n",
    "        n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    oof_preds = np.zeros(len(train_df))\n",
    "    valid_preds_ensemble = np.zeros(len(valid_df))\n",
    "    test_preds_ensemble = np.zeros(len(test_df))\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(\n",
    "        sgkf.split(X_train, y_train, groups=groups_for_split)\n",
    "    ):\n",
    "        print(f\"\\n  -> Processing Fold {fold + 1}/{N_SPLITS}...\")\n",
    "        X_train_fold, y_train_fold = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
    "        X_val_fold, y_val_fold = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        # Create Query IDs (qid) for the current fold\n",
    "        qid_train_fold = pd.factorize(\n",
    "            train_df.iloc[train_idx][\"id2\"].astype(str)\n",
    "            + \"_\"\n",
    "            + train_df.iloc[train_idx][\"id5\"].astype(str)\n",
    "        )[0]\n",
    "        qid_val_fold = pd.factorize(\n",
    "            train_df.iloc[val_idx][\"id2\"].astype(str)\n",
    "            + \"_\"\n",
    "            + train_df.iloc[val_idx][\"id5\"].astype(str)\n",
    "        )[0]\n",
    "        \n",
    "        # Create qids for the full validation and test sets for prediction\n",
    "        qid_valid_full = pd.factorize(\n",
    "            valid_df[\"id2\"].astype(str) + \"_\" + valid_df[\"id5\"].astype(str)\n",
    "        )[0]\n",
    "        qid_test_full = pd.factorize(\n",
    "            test_df[\"id2\"].astype(str) + \"_\" + test_df[\"id5\"].astype(str)\n",
    "        )[0]\n",
    "\n",
    "\n",
    "        model = xgb.XGBRanker(**xgb_params, n_estimators=2000)\n",
    "        model.fit(\n",
    "            X_train_fold,\n",
    "            y_train_fold,\n",
    "            qid=qid_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            eval_qid=[qid_val_fold],\n",
    "            callbacks=[xgb.callback.EarlyStopping(rounds=100, save_best=True)],\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        model_path = os.path.join(MODEL_DIR, f\"xgb_ranker_fold_{fold + 1}.json\")\n",
    "        model.save_model(model_path)\n",
    "        print(f\"  -> Model for fold {fold + 1} saved to '{model_path}'\")\n",
    "\n",
    "        oof_preds[val_idx] = model.predict(X_val_fold)\n",
    "        valid_preds_ensemble += model.predict(X_valid) / N_SPLITS\n",
    "        test_preds_ensemble += model.predict(X_test) / N_SPLITS\n",
    "\n",
    "    train_df[\"oof_xgb_prediction\"] = oof_preds\n",
    "    valid_df[\"oof_xgb_prediction\"] = valid_preds_ensemble\n",
    "    test_df[\"oof_xgb_prediction\"] = test_preds_ensemble\n",
    "    print(\"\\n  -> OOF and Ensemble predictions created successfully.\")\n",
    "\n",
    "    # --- Step 4: Save the final datasets ---\n",
    "    print(\"\\n--- Step 4: Saving new datasets with OOF features ---\")\n",
    "    train_df.to_parquet(f\"{OUTPUT_DIR}/train_5_oof_xgb.parquet\", index=False)\n",
    "    valid_df.to_parquet(f\"{OUTPUT_DIR}/valid_5_oof_xgb.parquet\", index=False)\n",
    "    test_df.to_parquet(f\"{OUTPUT_DIR}/test_5_oof_xgb.parquet\", index=False)\n",
    "\n",
    "    print(f\"  -> Saved 'train_5_oof_xgb.parquet' with shape {train_df.shape}\")\n",
    "    print(f\"  -> Saved 'valid_5_oof_xgb.parquet' with shape {valid_df.shape}\")\n",
    "    print(f\"  -> Saved 'test_5_oof_xgb.parquet' with shape {test_df.shape}\")\n",
    "\n",
    "    del train_df, valid_df, test_df, X_train, y_train, X_valid, X_test\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"\\n✅ --- XGBoost OOF Feature Generation Complete --- ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a341ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "# --- NEW DEPENDENCY: torch_scatter ---\n",
    "# This library is required for the vectorized listwise loss function.\n",
    "# Please install it using: pip install torch-scatter\n",
    "try:\n",
    "    from torch_scatter import scatter_softmax, scatter_sum\n",
    "except ImportError:\n",
    "    print(\"❌ WARNING: torch_scatter is not installed. The vectorized loss function will not work.\")\n",
    "    print(\"Please install it: pip install torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu121.html (adjust for your torch/cuda version)\")\n",
    "    # Define dummy functions if not installed to prevent immediate script crash\n",
    "    def scatter_softmax(src, index, dim): return src\n",
    "    def scatter_sum(src, index, dim): return src.sum()\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# 🚀 SCRIPT 2: RESIDUAL TRANSFORMER (WITH VECTORIZED LISTWISE RANKING LOSS)\n",
    "# =================================================================================\n",
    "\n",
    "# --- Competition Metric Calculation (Verified Correct) ---\n",
    "def calculate_map_at_7(y_true, y_pred, groups_df):\n",
    "    \"\"\"\n",
    "    Calculates Mean Average Precision @ 7, with the corrected denominator\n",
    "    to match the official competition scoring logic.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"y\": y_true, \"pred\": y_pred})\n",
    "    df = pd.concat([groups_df.reset_index(drop=True), df], axis=1)\n",
    "    df = df.sort_values([\"id2\", \"id5\", \"pred\"], ascending=[True, True, False])\n",
    "    df[\"position\"] = df.groupby([\"id2\", \"id5\"]).cumcount() + 1\n",
    "    df_top7 = df[df['position'] <= 7].copy()\n",
    "    df_top7['cumsum_y'] = df_top7.groupby([\"id2\", \"id5\"])['y'].cumsum()\n",
    "    df_top7['ap_numerator_term'] = (df_top7['cumsum_y'] / df_top7['position']) * df_top7['y']\n",
    "    ap_numerators = df_top7.groupby([\"id2\", \"id5\"])['ap_numerator_term'].sum()\n",
    "    ap_denominators = df.groupby([\"id2\", \"id5\"])['y'].sum().clip(upper=7)\n",
    "    ap_df = pd.concat([ap_numerators, ap_denominators], axis=1).fillna(0)\n",
    "    ap_df.columns = ['map_numerator', 'map_denominator']\n",
    "    ap_df['ap_at_7'] = ap_df.apply(\n",
    "        lambda row: row[\"map_numerator\"] / row[\"map_denominator\"] if row[\"map_denominator\"] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    return ap_df[\"ap_at_7\"].mean()\n",
    "\n",
    "# --- Advanced Transformer Configuration ---\n",
    "class TransformerConfig:\n",
    "    # Use the output from the XGBoost OOF script as input\n",
    "    TRAIN_FILE = \"inter/train_5_oof_xgb.parquet\"\n",
    "    TEST_FILE = \"inter/test_5_oof_xgb.parquet\"\n",
    "    \n",
    "    CAT_EMBED_DIM = 8\n",
    "    EMBED_DIM = 192\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 4\n",
    "    DIM_FEEDFORWARD = 512\n",
    "    DROPOUT = 0.2\n",
    "    MAX_SEQ_LEN = 20\n",
    "    \n",
    "    EPOCHS = 15\n",
    "    N_SPLITS = 5\n",
    "    LR = 5e-5\n",
    "    BATCH_SIZE = 256\n",
    "    NUM_WORKERS = 2\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    MODEL_OUTPUT_PATH = \"model/transformer_residual_model\"\n",
    "\n",
    "# --- PyTorch Dataset and Model (Unchanged from previous version) ---\n",
    "class AmexAdvancedDataset(Dataset):\n",
    "    def _init_(self, sequences_cat, sequences_num, labels, gbdt_preds=None):\n",
    "        self.sequences_cat = torch.tensor(sequences_cat, dtype=torch.long)\n",
    "        self.sequences_num = torch.tensor(sequences_num, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        if gbdt_preds is None:\n",
    "            gbdt_preds = np.zeros_like(labels)\n",
    "        self.gbdt_preds = torch.tensor(gbdt_preds, dtype=torch.float32)\n",
    "\n",
    "    def _len_(self):\n",
    "        return len(self.sequences_cat)\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        return self.sequences_cat[idx], self.sequences_num[idx], self.labels[idx], self.gbdt_preds[idx]\n",
    "\n",
    "\n",
    "class AmexPureTransformer(nn.Module):\n",
    "    def _init_(self, config, num_numerical_features, cat_cardinalities):\n",
    "        super()._init_()\n",
    "        self.config = config\n",
    "        self.cat_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cardinality, config.CAT_EMBED_DIM) for cardinality in cat_cardinalities\n",
    "        ])\n",
    "        total_cat_embed_dim = len(cat_cardinalities) * config.CAT_EMBED_DIM\n",
    "        self.num_norm = nn.LayerNorm(num_numerical_features)\n",
    "        combined_feature_dim = total_cat_embed_dim + num_numerical_features\n",
    "        self.feature_proj = nn.Linear(combined_feature_dim, config.EMBED_DIM)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, config.MAX_SEQ_LEN, config.EMBED_DIM))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=config.EMBED_DIM, nhead=config.NUM_HEADS,\n",
    "            dim_feedforward=config.DIM_FEEDFORWARD, dropout=config.DROPOUT,\n",
    "            activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=config.NUM_LAYERS)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(config.EMBED_DIM),\n",
    "            nn.Linear(config.EMBED_DIM, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        cat_embeds = [self.cat_embeddings[i](x_cat[:, :, i]) for i in range(x_cat.size(2))]\n",
    "        all_cat_embeds = torch.cat(cat_embeds, dim=-1)\n",
    "        x_num_norm = self.num_norm(x_num)\n",
    "        x = torch.cat([all_cat_embeds, x_num_norm], dim=-1)\n",
    "        x = self.feature_proj(x)\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# --- Data Preparation and Training Functions ---\n",
    "def prepare_advanced_transformer_data(config):\n",
    "    print(\"\\n--- Preparing data for Advanced Transformer (Residual Modeling) ---\")\n",
    "    \n",
    "    train_df = pd.read_parquet(config.TRAIN_FILE)\n",
    "    test_df = pd.read_parquet(config.TEST_FILE)\n",
    "    \n",
    "    print(\"  -> Creating blended GBDT prediction and residual target...\")\n",
    "    train_df['gbdt_blend_oof'] = (train_df['oof_lgbm_prediction'] + train_df['oof_xgb_prediction']) / 2.0\n",
    "    train_df['residual_target'] = train_df['y'] - train_df['gbdt_blend_oof']\n",
    "    \n",
    "    with open(os.path.join(\"model\", \"top_200_features.json\"), 'r') as f:\n",
    "        top_features = json.load(f)\n",
    "    with open(os.path.join(\"model\", \"all_categorical_features.json\"), 'r') as f:\n",
    "        all_categorical_features = json.load(f)\n",
    "\n",
    "    categorical_cols = [col for col in top_features if col in all_categorical_features]\n",
    "    numerical_cols = [col for col in top_features if col not in categorical_cols]\n",
    "    \n",
    "    if 'oof_lgbm_prediction' not in numerical_cols:\n",
    "        numerical_cols.append('oof_lgbm_prediction')\n",
    "    if 'oof_xgb_prediction' not in numerical_cols:\n",
    "        numerical_cols.append('oof_xgb_prediction')\n",
    "\n",
    "    cat_cardinalities = []\n",
    "    for col in tqdm(categorical_cols, desc=\"  -> Encoding Categoricals (Leakage-Free)\"):\n",
    "        learned_categories = train_df[col].astype('category').cat.categories\n",
    "        mapping = {cat: code for code, cat in enumerate(learned_categories, 1)}\n",
    "        train_df[col] = train_df[col].map(mapping).fillna(0).astype(int)\n",
    "        test_df[col] = test_df[col].map(mapping).fillna(0).astype(int)\n",
    "        cat_cardinalities.append(len(mapping) + 1)\n",
    "\n",
    "    def create_sequences(df, cols_cat, cols_num, lbl_col=None, is_test=False):\n",
    "        sequences_cat, sequences_num, labels, customer_ids = [], [], [], []\n",
    "        grouped = df.groupby('id2')\n",
    "        for cust_id, group in tqdm(grouped, desc=f\"Creating sequences ({'test' if is_test else 'train'})\"):\n",
    "            cat_feats, num_feats = group[cols_cat].values, group[cols_num].values\n",
    "            padding_len = config.MAX_SEQ_LEN - len(group)\n",
    "            if padding_len > 0:\n",
    "                cat_feats = np.pad(cat_feats, ((padding_len, 0), (0, 0)), 'constant', constant_values=0)\n",
    "                num_feats = np.pad(num_feats, ((padding_len, 0), (0, 0)), 'constant', constant_values=0)\n",
    "            else:\n",
    "                cat_feats, num_feats = cat_feats[-config.MAX_SEQ_LEN:], num_feats[-config.MAX_SEQ_LEN:]\n",
    "            sequences_cat.append(cat_feats)\n",
    "            sequences_num.append(num_feats)\n",
    "            customer_ids.append(cust_id)\n",
    "            if not is_test:\n",
    "                group_labels = group[lbl_col].values\n",
    "                if padding_len > 0:\n",
    "                    padded_labels = np.pad(group_labels, (padding_len, 0), 'constant', constant_values=-100.0)\n",
    "                else:\n",
    "                    padded_labels = group_labels[-config.MAX_SEQ_LEN:]\n",
    "                labels.append(padded_labels)\n",
    "        final_cat, final_num = np.array(sequences_cat), np.nan_to_num(np.array(sequences_num))\n",
    "        if is_test:\n",
    "            return final_cat, final_num, np.array(customer_ids)\n",
    "        return final_cat, final_num, np.array(labels), np.array(customer_ids)\n",
    "\n",
    "    cat_train, num_train, lbl_train, _ = create_sequences(train_df, categorical_cols, numerical_cols, 'residual_target')\n",
    "    cat_test, num_test, _ = create_sequences(test_df, categorical_cols, numerical_cols, is_test=True)\n",
    "    \n",
    "    train_data = (cat_train, num_train, lbl_train, train_df)\n",
    "    test_data = (cat_test, num_test, test_df)\n",
    "    \n",
    "    return train_data, test_data, len(numerical_cols), cat_cardinalities\n",
    "\n",
    "### --- UPDATED: VECTORIZED LISTWISE LOSS FUNCTION --- ###\n",
    "def train_one_epoch_listwise_vectorized(model, dataloader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using a fully vectorized ListNet loss calculation,\n",
    "    which is significantly faster on GPU than the previous iterative approach.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training (Vectorized Listwise)\", leave=False):\n",
    "        cat_seq, num_seq, labels, gbdt_preds = batch[0].to(device), batch[1].to(device), batch[2].to(device), batch[3].to(device)\n",
    "\n",
    "        # Get model predictions for the residuals\n",
    "        residual_preds = model(cat_seq, num_seq).squeeze(-1)\n",
    "\n",
    "        # --- Vectorized Loss Calculation ---\n",
    "        # Create a mask to identify non-padded elements\n",
    "        mask = (labels != -100.0)\n",
    "        \n",
    "        # Create a batch index tensor to identify which items belong to which sequence\n",
    "        # Example: [0, 0, 0, 1, 1, 2, 2, 2, 2, ...]\n",
    "        batch_idx = torch.arange(labels.size(0), device=device).unsqueeze(1).expand_as(labels)\n",
    "        \n",
    "        # Flatten all tensors and apply the mask to keep only valid (non-padded) items\n",
    "        flat_batch_idx = batch_idx[mask]\n",
    "        \n",
    "        # If there are no valid labels in the batch, skip to the next one\n",
    "        if flat_batch_idx.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        # Get the corresponding predictions and labels for the valid items\n",
    "        flat_residual_preds = residual_preds[mask]\n",
    "        flat_gbdt_preds = gbdt_preds[mask]\n",
    "        flat_labels = labels[mask]\n",
    "\n",
    "        # Reconstruct the final predictions and true labels in their flattened 1D form\n",
    "        flat_final_preds = flat_gbdt_preds + flat_residual_preds\n",
    "        flat_y_true = flat_gbdt_preds + flat_labels\n",
    "\n",
    "        # Calculate the \"ideal\" probability distribution from true labels for each sequence\n",
    "        # This performs a grouped softmax based on the batch index\n",
    "        y_prob_dist = scatter_softmax(flat_y_true, flat_batch_idx, dim=0)\n",
    "        \n",
    "        # Calculate the model's predicted log-probability distribution for each sequence\n",
    "        log_pred_prob_dist = scatter_softmax(flat_final_preds, flat_batch_idx, dim=0).log()\n",
    "\n",
    "        # Calculate the KL Divergence term for each individual item\n",
    "        kl_div_terms = - (y_prob_dist * log_pred_prob_dist)\n",
    "\n",
    "        # Sum the terms for each sequence using scatter_sum to get the loss per sequence\n",
    "        sequence_losses = scatter_sum(kl_div_terms, flat_batch_idx, dim=0)\n",
    "\n",
    "        # Calculate the mean loss over all sequences in the batch\n",
    "        mean_loss = sequence_losses.mean()\n",
    "\n",
    "        # --- Standard PyTorch optimization step ---\n",
    "        optimizer.zero_grad()\n",
    "        mean_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += mean_loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def predict_transformer(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Predicting\", leave=False):\n",
    "            outputs = model(batch[0].to(device), batch[1].to(device)).squeeze(-1)\n",
    "            # Create a mask to filter out predictions for padded items\n",
    "            mask = (batch[0][:, :, 0] != 0).to(device) \n",
    "            for i in range(outputs.shape[0]):\n",
    "                # Only keep predictions for the original, non-padded sequence items\n",
    "                predictions.extend(outputs[i][mask[i]].cpu().numpy())\n",
    "    return np.array(predictions)\n",
    "\n",
    "def evaluate_model(model, dataloader, device, ids_df, gbdt_blend_oof_fold):\n",
    "    model.eval()\n",
    "    residual_preds = predict_transformer(model, dataloader, device)\n",
    "    eval_df = ids_df.copy()\n",
    "    \n",
    "    if len(eval_df) != len(residual_preds):\n",
    "        print(f\"CRITICAL ERROR: Length mismatch. Events: {len(eval_df)}, Preds: {len(residual_preds)}\")\n",
    "        return 0.0\n",
    "\n",
    "    eval_df['residual_pred'] = residual_preds\n",
    "    eval_df['gbdt_blend_pred'] = gbdt_blend_oof_fold\n",
    "    eval_df['final_pred'] = eval_df['gbdt_blend_pred'] + eval_df['residual_pred']\n",
    "    \n",
    "    score = calculate_map_at_7(eval_df['y'].values, eval_df['final_pred'].values, eval_df[['id2', 'id5']])\n",
    "    return score\n",
    "\n",
    "def run_stage2_transformer():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 === SCRIPT 2: RESIDUAL TRANSFORMER PIPELINE (VECTORIZED LISTWISE RANKING LOSS) ===\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    config = TransformerConfig()\n",
    "    os.makedirs(os.path.dirname(config.MODEL_OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "    (train_data, test_data, num_numerical, cat_cards) = prepare_advanced_transformer_data(config)\n",
    "    cat_train_full, num_train_full, lbl_train_full, original_train_df = train_data\n",
    "    cat_test, num_test, original_test_df = test_data\n",
    "\n",
    "    # Group by customer ID to prepare for stratified k-fold splitting\n",
    "    customer_y_stratify = original_train_df.groupby('id2')['y'].max()\n",
    "\n",
    "    print(\"\\n--- Creating GBDT prediction sequences for ranking loss ---\")\n",
    "    padded_gbdt_preds = []\n",
    "    for cust_id in tqdm(original_train_df['id2'].unique(), desc=\"Padding GBDT Preds\"):\n",
    "        cust_preds = original_train_df[original_train_df['id2'] == cust_id]['gbdt_blend_oof'].values\n",
    "        padding_len = config.MAX_SEQ_LEN - len(cust_preds)\n",
    "        if padding_len > 0:\n",
    "            padded_preds = np.pad(cust_preds, (padding_len, 0), 'constant', constant_values=0)\n",
    "        else:\n",
    "            padded_preds = cust_preds[-config.MAX_SEQ_LEN:]\n",
    "        padded_gbdt_preds.append(padded_preds)\n",
    "    gbdt_preds_seq = np.array(padded_gbdt_preds)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=config.N_SPLITS, shuffle=True, random_state=42)\n",
    "    oof_residual_preds = np.full(len(original_train_df), np.nan)\n",
    "    test_preds_list = []\n",
    "\n",
    "    for fold, (train_cust_idx, val_cust_idx) in enumerate(skf.split(np.zeros(len(customer_y_stratify)), customer_y_stratify)):\n",
    "        print(f\"\\n{'='*30} FOLD {fold+1}/{config.N_SPLITS} {'='*30}\")\n",
    "\n",
    "        # Get the actual customer IDs for the validation fold\n",
    "        val_cust_ids_fold = customer_y_stratify.index[val_cust_idx]\n",
    "        # Create masks to select all events belonging to these customers\n",
    "        val_events_mask = original_train_df['id2'].isin(val_cust_ids_fold)\n",
    "        val_events_indices = original_train_df[val_events_mask].index\n",
    "\n",
    "        # Split sequence data by customer indices\n",
    "        X_cat_train, X_num_train, y_res_train = cat_train_full[train_cust_idx], num_train_full[train_cust_idx], lbl_train_full[train_cust_idx]\n",
    "        X_cat_val, X_num_val, y_res_val = cat_train_full[val_cust_idx], num_train_full[val_cust_idx], lbl_train_full[val_cust_idx]\n",
    "        X_gbdt_train, X_gbdt_val = gbdt_preds_seq[train_cust_idx], gbdt_preds_seq[val_cust_idx]\n",
    "        \n",
    "        # Prepare data for evaluation\n",
    "        ids_val_fold_df = original_train_df.loc[val_events_indices]\n",
    "        gbdt_blend_oof_val_fold = (ids_val_fold_df['oof_lgbm_prediction'] + ids_val_fold_df['oof_xgb_prediction']) / 2.0\n",
    "\n",
    "        loader_train = DataLoader(AmexAdvancedDataset(X_cat_train, X_num_train, y_res_train, X_gbdt_train), batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS)\n",
    "        loader_val = DataLoader(AmexAdvancedDataset(X_cat_val, X_num_val, y_res_val, X_gbdt_val), batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n",
    "\n",
    "        model = AmexPureTransformer(config, num_numerical, cat_cards).to(config.DEVICE)\n",
    "        optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=1e-2)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "        best_map_score = -1.0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(config.EPOCHS):\n",
    "            # --- MODIFIED: Call the new vectorized training function ---\n",
    "            train_loss = train_one_epoch_listwise_vectorized(model, loader_train, optimizer, config.DEVICE)\n",
    "            \n",
    "            map_score = evaluate_model(model, loader_val, config.DEVICE, ids_val_fold_df, gbdt_blend_oof_val_fold.values)\n",
    "            print(f\"Epoch {epoch+1}/{config.EPOCHS} -> Train Loss: {train_loss:.6f} | Val MAP@7: {map_score:.6f}\")\n",
    "            \n",
    "            scheduler.step(map_score)\n",
    "\n",
    "            if map_score > best_map_score:\n",
    "                best_map_score = map_score\n",
    "                torch.save(model.state_dict(), f\"{config.MODEL_OUTPUT_PATH}fold{fold+1}.pth\")\n",
    "                print(f\"  -> 🎉 New best score! Model for Fold {fold+1} saved.\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= 4:\n",
    "                print(\"  -> Stopping early due to lack of improvement.\")\n",
    "                break\n",
    "\n",
    "        print(f\"-> Loading best model for Fold {fold+1} (MAP@7: {best_map_score:.6f}) for final predictions...\")\n",
    "        model.load_state_dict(torch.load(f\"{config.MODEL_OUTPUT_PATH}fold{fold+1}.pth\"))\n",
    "        \n",
    "        # Predict on the validation part of this fold to generate OOF predictions\n",
    "        val_preds_fold = predict_transformer(model, loader_val, config.DEVICE)\n",
    "        oof_residual_preds[val_events_indices] = val_preds_fold\n",
    "        \n",
    "        # Predict on the full test set\n",
    "        test_loader = DataLoader(AmexAdvancedDataset(cat_test, num_test, np.zeros((len(cat_test), config.MAX_SEQ_LEN))), batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS)\n",
    "        test_preds_fold = predict_transformer(model, test_loader, config.DEVICE)\n",
    "        test_preds_list.append(test_preds_fold)\n",
    "        \n",
    "        del model, loader_train, loader_val; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n--- K-Fold Training Finished. Saving final OOF and Test predictions. ---\")\n",
    "    \n",
    "    final_train_df = pd.read_parquet(config.TRAIN_FILE)\n",
    "    final_train_df['oof_transformer_residual'] = oof_residual_preds\n",
    "    final_train_df.to_parquet(\"inter/train_6_final_ensemble.parquet\", index=False)\n",
    "    print(\"✅ Saved 'train_6_final_ensemble.parquet' with Transformer OOF residual predictions.\")\n",
    "    \n",
    "    avg_test_preds = np.mean(test_preds_list, axis=0)\n",
    "    final_test_df = pd.read_parquet(config.TEST_FILE)\n",
    "    final_test_df['transformer_residual'] = avg_test_preds\n",
    "    final_test_df.to_parquet(\"inter/test_6_final_ensemble.parquet\", index=False)\n",
    "    print(\"✅ Saved 'test_6_final_ensemble.parquet' with averaged Transformer test residual predictions.\")\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    # Run the Transformer stage\n",
    "    run_stage2_transformer()\n",
    "\n",
    "# =================================================================================\n",
    "#   SCRIPT 3: FINAL ENSEMBLING AND SUBMISSION (NO CHANGES NEEDED)\n",
    "# =================================================================================\n",
    "def run_final_ensembling():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 === SCRIPT 3: FINAL LOGISTIC ENSEMBLING & SUBMISSION ===\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        train_full = pd.read_parquet(\"inter/train_6_final_ensemble.parquet\")\n",
    "        test_full = pd.read_parquet(\"inter/test_6_final_ensemble.parquet\")\n",
    "        print(f\"Train data shape: {train_full.shape}\")\n",
    "        print(f\"Test data shape: {test_full.shape}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ ERROR: Could not find an input file. Please ensure all previous model stages have run. Details: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Training Logistic Regression to find optimal blending weights ---\")\n",
    "    \n",
    "    ensemble_features = ['oof_lgbm_prediction', 'oof_xgb_prediction', 'oof_transformer_residual']\n",
    "    X_ensemble_train = train_full[ensemble_features].fillna(0)\n",
    "    y_ensemble_train = train_full['y']\n",
    "\n",
    "    ensemble_model = LogisticRegression(class_weight='balanced', C=0.1, solver='liblinear')\n",
    "    ensemble_model.fit(X_ensemble_train, y_ensemble_train)\n",
    "    \n",
    "    print(\"✅ Ensemble model trained.\")\n",
    "    print(f\"Learned Coefficients (Weights):\")\n",
    "    for feature, coef in zip(ensemble_features, ensemble_model.coef_[0]):\n",
    "        print(f\"  - {feature}: {coef:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Generating final predictions on the test set ---\")\n",
    "    \n",
    "    # Select the same feature columns from the test set for prediction\n",
    "    X_ensemble_test = test_full[ensemble_features].copy().fillna(0)\n",
    "\n",
    "    final_predictions_prob = ensemble_model.predict_proba(X_ensemble_test)[:, 1]\n",
    "\n",
    "    print(\"\\n--- Creating final submission file ---\")\n",
    "    \n",
    "    submission_df = test_full[['id1', 'id2', 'id3', 'id5']].copy()\n",
    "    submission_df['pred'] = final_predictions_prob\n",
    "    submission_df['id5'] = pd.to_datetime(submission_df['id5']).dt.date\n",
    "    \n",
    "    output_path = 'final_ensemble_submission.csv'\n",
    "    submission_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n🎉🎉 Final ensemble submission file created successfully: '{output_path}' 🎉🎉\")\n",
    "    print(\"Submission Head:\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    # Run the final ensembling and create submission\n",
    "    run_final_ensembling()\n",
    "\n",
    "# =================================================================================\n",
    "# 🚀 SCRIPT 4: FEATURE IMPORTANCE REPORT (NO CHANGES NEEDED)\n",
    "# =================================================================================\n",
    "def generate_feature_importance_report():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 === SCRIPT 4: Generating Feature Importance Report ===\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    MODEL_DIR = \"model\"\n",
    "    N_SPLITS = 5\n",
    "    all_importances = []\n",
    "    \n",
    "    print(f\"--- Loading {N_SPLITS} fold models to average importance ---\")\n",
    "    \n",
    "    try:\n",
    "        with open(os.path.join(MODEL_DIR, \"top_200_features.json\"), \"r\") as f:\n",
    "            top_features = json.load(f)\n",
    "\n",
    "        for fold in range(1, N_SPLITS + 1):\n",
    "            model_path = os.path.join(MODEL_DIR, f\"lgbm_ranker_fold_{fold}.txt\")\n",
    "            model = lgb.Booster(model_file=model_path)\n",
    "            importance_gain = model.feature_importance(importance_type='gain')\n",
    "            fold_importance_df = pd.DataFrame({\n",
    "                'feature': top_features,\n",
    "                f'importance_gain_fold_{fold}': importance_gain\n",
    "            })\n",
    "            all_importances.append(fold_importance_df)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error: Could not find a model file. Details: {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return\n",
    "\n",
    "    if not all_importances:\n",
    "        print(\"No importance data found.\")\n",
    "        return\n",
    "\n",
    "    final_importance_df = all_importances[0]\n",
    "    for i in range(1, len(all_importances)):\n",
    "        final_importance_df = pd.merge(final_importance_df, all_importances[i], on='feature', how='outer')\n",
    "\n",
    "    gain_cols = [col for col in final_importance_df.columns if 'importance_gain' in col]\n",
    "    final_importance_df['average_gain_importance'] = final_importance_df[gain_cols].mean(axis=1)\n",
    "\n",
    "    report_df = final_importance_df[['feature', 'average_gain_importance']].sort_values(\n",
    "        by='average_gain_importance', ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    output_path = 'feature_importance_report.xlsx'\n",
    "    report_df.to_excel(output_path, index=False, engine='openpyxl')\n",
    "    \n",
    "    print(f\"\\n✅ Feature importance report saved successfully to '{output_path}'\")\n",
    "    print(\"--- Top 10 Most Important Features ---\")\n",
    "    print(report_df.head(10))\n",
    "\n",
    "# =================================================================================\n",
    "# --- MAIN EXECUTION BLOCK ---\n",
    "# =================================================================================\n",
    "if _name_ == \"_main_\":\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    # Generate the feature importance report from the GBDT models\n",
    "    generate_feature_importance_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff0d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a20e9b-953d-4163-876a-2b226c8370ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (final)",
   "language": "python",
   "name": "final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
